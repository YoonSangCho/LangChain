{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. LLM, GPT, & Chat GPT API 사용하기**\n",
    "- openAI key 생성 URL: https://platform.openai.com/api-keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip  install openai langchain unstructured pypdf pdf2image docx2txt pdfminer tiktoken sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "openai_api_key = 'Your KEY'\n",
    "os.environ[\"OPENAI_API_KEY\"]=openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer_gpt \n",
      "\n",
      "The most popular large language model is currently GPT-3 (Generative Pre-trained Transformer-3), developed by OpenAI.\n"
     ]
    }
   ],
   "source": [
    "# GPT\n",
    "gpt = OpenAI(model_name='gpt-3.5-turbo-instruct')\n",
    "answer_gpt = gpt.predict(\"What is the most popular Large language model?\")\n",
    "print(\"answer_gpt\", answer_gpt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chatGPT 가장 인기 있는 대형 언어 모델은 GPT-3입니다.\n"
     ]
    }
   ],
   "source": [
    "# ChatGPT: 답변을 한국어로 받아보기\n",
    "chatgpt = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "answer_chatgpt = chatgpt.predict(\"What is the most popular Large language model? answer in Korean\")\n",
    "print(\"chatGPT\", answer_chatgpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here are some popular places to visit in Korea:\n",
      "\n",
      "1. Seoul: The capital city of Korea, Seoul is a bustling metropolis with a mix of modern skyscrapers and historic palaces. Don't miss visiting Gyeongbokgung Palace, Bukchon Hanok Village, and N Seoul Tower.\n",
      "\n",
      "2. Jeju Island: Known for its stunning natural landscapes, Jeju Island is a popular destination for hiking, beaches, and exploring lava tubes and waterfalls.\n",
      "\n",
      "3. Busan: Korea's second-largest city, Busan is famous for its beaches, seafood markets, and vibrant nightlife. Be sure to visit Haeundae Beach, Gamcheon Culture Village, and Jagalchi Fish Market.\n",
      "\n",
      "4. Gyeongju: Often referred to as the \"museum without walls,\" Gyeongju is a city filled with ancient temples, burial mounds, and historic sites. Don't miss visiting Bulguksa Temple, Seokguram Grotto, and the Royal Tombs.\n",
      "\n",
      "5. Incheon: Located just outside of Seoul, Incheon is a port city known for its Chinatown, Songdo International Business District, and Incheon Grand Park.\n",
      "\n",
      "These are just a few of the many amazing places to visit in Korea. Enjoy your trip!\n",
      "Of course! Here are some recommendations for places to visit in Korea:\n",
      "\n",
      "1. Seoul: The bustling capital city of Korea offers a mix of modern and traditional attractions, including historic palaces, shopping districts, and vibrant markets.\n",
      "\n",
      "2. Jeju Island: Located off the southern coast of Korea, Jeju Island is known for its beautiful beaches, volcanic landscapes, and unique cultural sites.\n",
      "\n",
      "3. Busan: Korea's second-largest city is famous for its beaches, vibrant food scene, and cultural attractions such as the bustling Jagalchi Fish Market.\n",
      "\n",
      "4. Gyeongju: Known as the \"Museum Without Walls,\" Gyeongju is a UNESCO World Heritage site filled with ancient temples, tombs, and historical landmarks.\n",
      "\n",
      "5. Bukhansan National Park: Located just outside of Seoul, this national park offers stunning mountain scenery, hiking trails, and opportunities for outdoor activities.\n",
      "\n",
      "6. Andong: A picturesque city known for its traditional Korean architecture, Andong is home to the historic Hahoe Folk Village and the Andong Mask Dance Festival.\n",
      "\n",
      "7. Incheon: The port city of Incheon is home to modern landmarks such as the Songdo International Business District, as well as cultural attractions like Chinatown and the Incheon Grand Park.\n",
      "\n",
      "These are just a few of the many incredible places to visit in Korea. Enjoy your trip!\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning\n",
    "# chatgpt 답변 일관성 0: 일관적-default, 1: 같은 질문 받더라도 다르게 무작위성이 부여됨, 2: 가장 창의적)\n",
    "\n",
    "chatgpt_t0 = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "answer_t0 = chatgpt_t0.predict(\"Could you recommend good places we should go in Korea?\")\n",
    "print(answer_t0)\n",
    "\n",
    "chatgpt_t1 = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1)\n",
    "answer_t1 = chatgpt_t1.predict(\"Could you recommend good places we should go in Korea?\")\n",
    "print(answer_t1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Seoul: Visit iconic landmarks such as Gyeongbokgung Palace, N Seoul Tower, and Bukchon Hanok Village. Don't miss out on exploring vibrant neighborhoods like Myeongdong and Insadong.\n",
      "\n",
      "2. Jeju Island: Known for its stunning natural beauty, Jeju Island offers an array of attractions including Seongsan Ilchulbong Peak, Manjanggul Cave, and Jeju Loveland.\n",
      "\n",
      "3. Busan: Explore the bustling city of Busan and enjoy the beautiful Haeundae Beach, Gamcheon Culture Village, and Beomeosa Temple.\n",
      "\n",
      "4. Gyeongju: Immerse yourself in Korea's ancient history by visiting historic sites such as Bulguksa Temple, Seokguram Grotto, and the Royal Tombs in Gyeongju.\n",
      "\n",
      "5. Incheon: Discover the vibrant port city of Incheon and visit attractions such as the Songdo Central Park, Incheon Grand Park, and Wolmido Island.\n",
      "\n",
      "6. Andong: Experience traditional Korean culture in Andong by visiting Hahoe Folk Village, Dosan Seowon Confucian Academy, and the Andong Mask Dance Festival.\n",
      "\n",
      "7. Pyeongchang: Enjoy outdoor activities in Pyeongchang, host city of the 2018 Winter Olympics, with skiing, snowboarding, and hiking opportunities.\n",
      "\n",
      "These are just a few of the countless amazing places to visit in Korea. Each region offers something unique and memorable, so make sure to explore as much as possible during your trip."
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"1. Seoul: Visit iconic landmarks such as Gyeongbokgung Palace, N Seoul Tower, and Bukchon Hanok Village. Don't miss out on exploring vibrant neighborhoods like Myeongdong and Insadong.\\n\\n2. Jeju Island: Known for its stunning natural beauty, Jeju Island offers an array of attractions including Seongsan Ilchulbong Peak, Manjanggul Cave, and Jeju Loveland.\\n\\n3. Busan: Explore the bustling city of Busan and enjoy the beautiful Haeundae Beach, Gamcheon Culture Village, and Beomeosa Temple.\\n\\n4. Gyeongju: Immerse yourself in Korea's ancient history by visiting historic sites such as Bulguksa Temple, Seokguram Grotto, and the Royal Tombs in Gyeongju.\\n\\n5. Incheon: Discover the vibrant port city of Incheon and visit attractions such as the Songdo Central Park, Incheon Grand Park, and Wolmido Island.\\n\\n6. Andong: Experience traditional Korean culture in Andong by visiting Hahoe Folk Village, Dosan Seowon Confucian Academy, and the Andong Mask Dance Festival.\\n\\n7. Pyeongchang: Enjoy outdoor activities in Pyeongchang, host city of the 2018 Winter Olympics, with skiing, snowboarding, and hiking opportunities.\\n\\nThese are just a few of the countless amazing places to visit in Korea. Each region offers something unique and memorable, so make sure to explore as much as possible during your trip.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 실시간으로 답변 받아보기: streaming=True\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "chatgpt = ChatOpenAI(model_name='gpt-3.5-turbo', streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=1)\n",
    "chatgpt.predict(\"Could you recommend good places we should go in Korea?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I recommend visiting the beautiful city of Paris in France. Known as the \"City of Light,\" Paris is famous for its iconic landmarks such as the Eiffel Tower, Louvre Museum, Notre-Dame Cathedral, and Champs-Élysées. You can also explore charming neighborhoods like Montmartre, enjoy delicious French cuisine at local cafes, and experience the vibrant art and culture scene. Don\\'t forget to take a leisurely stroll along the Seine River and immerse yourself in the romantic atmosphere of this enchanting city.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ChatGPT에 역할 부여하기\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "chatgpt = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "message = [\n",
    "    # ChatGPT 역할을 부여\n",
    "    SystemMessage(\n",
    "        content=\"You are a world tour guide. Please recommend a good place from where I mentioned it\"\n",
    "    ),\n",
    "    # 이 문장에 대해 역할대로 수행\n",
    "    HumanMessage(\n",
    "        content=\"I want to go France\"\n",
    "    )\n",
    "]\n",
    "response_langchain = chatgpt(message)\n",
    "response_langchain.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Prompt 실습**\n",
    "- model에 대한 입력을 구성, LangChain은 이 프롬프트를 쉽게 구성하고 작업할 수 있도록 여러 클래스와 함수를 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string_prompt_value text='good places in seoul'\n",
      "chat_prompt_value messages=[HumanMessage(content='good places in seoul')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "# 일반 LLM\n",
    "string_prompt = PromptTemplate.from_template(\"good places in {subject}\") # 프롬프트 템플릿을 통해 매개변수로 삽입 가능한 문자열로 변환\n",
    "string_prompt_value = string_prompt.format_prompt(subject=\"seoul\")\n",
    "print(\"string_prompt_value\", string_prompt_value)\n",
    "# Chat LLM\n",
    "chat_prompt = ChatPromptTemplate.from_template(\"good places in {subject}\")\n",
    "chat_prompt_value = chat_prompt.format_prompt(subject=\"seoul\")\n",
    "print(\"chat_prompt_value\", chat_prompt_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GPT와 PromptTemplate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "You are a world tour guide. Please recommend a good place from where I mentioned it\n",
      "\n",
      "<places>\n",
      "korea, paris, new york, london\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "template = \"\"\"\n",
    "\n",
    "You are a world tour guide. Please recommend a good place from where I mentioned it\n",
    "\n",
    "<places>\n",
    "{places}\n",
    "\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=['places'],\n",
    "    template = template\n",
    ")\n",
    "# 여러개 질문 가능한 템플릿\n",
    "print(prompt_template.format(places='korea, paris, new york, london'))\n",
    "print(gpt(prompt_template.format(places='korea, paris, new york, london')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ChatGPT와 ChatPromptTemplate**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would recommend visiting Paris, France. Paris is known for its beautiful architecture, world-class museums like the Louvre, iconic landmarks such as the Eiffel Tower, and delicious cuisine. Stroll along the Seine River, explore charming neighborhoods like Montmartre, and immerse yourself in the rich culture and history of this romantic city. Don't forget to indulge in some pastries and coffee at a cozy café while people-watching!\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate, SystemMessagePromptTemplate, AIMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# chatGPT 모델\n",
    "chatgpt = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "# chatGPT 역할\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "# 사용자 입력 \n",
    "human_template = \"{places}\"\n",
    "human_message_promt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_promt])\n",
    "answer = chatgpt(chat_prompt.format_prompt(places='korea, paris, new york, london').to_messages())\n",
    "print(answer.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Document Loaders 실습하기**\n",
    "\n",
    "- web 에 있는 Text 불러오기\n",
    "- pdf 에 있는 Text 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Why does Biden keep mentioning January 6? Because Trump won‚Äôt stop talking about it | CNN Politics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CNN values your feedback\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                        1. How relevant is this ad to you?\n",
      "                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                2. Did you encounter any technical issues?\n",
      "                                        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                        Video player was slow to load content\n",
      "                                                                        \n",
      "\n",
      "\n",
      "\n",
      "                                                                        Video content never loaded\n",
      "                                                                        \n",
      "\n",
      "\n",
      "\n",
      "                                                                        Ad froze or did not finish loading\n",
      "                                                                        \n",
      "\n",
      "\n",
      "\n",
      "                                                                        Video content did not start after ad\n",
      "                                                                        \n",
      "\n",
      "\n",
      "\n",
      "                                                                        Audio on ad was too loud\n",
      "                                                                        \n",
      "\n",
      "\n",
      "\n",
      "                                                                        Other issues\n",
      "                                                                        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                        Ad never loaded\n",
      "                                                                        \n",
      "\n",
      "\n",
      "\n",
      "                                                                        Ad prevented/slowed the page from loading\n",
      "                                                                        \n",
      "\n",
      "\n",
      "\n",
      "                                                                        Content moved around while ad loaded\n",
      "                                                                        \n",
      "\n",
      "\n",
      "\n",
      "                                                                        Ad was repetitive to ads I've seen previously\n",
      "                                                                        \n",
      "\n",
      "\n",
      "\n",
      "                                                                        Other issues\n",
      "                                                                        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                        Cancel\n",
      "                                                \n",
      "\n",
      "                                                        Submit\n",
      "                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Thank You!\n",
      "\n",
      "                                        Your effort and contribution in providing this feedback is much\n",
      "                                        appreciated.\n",
      "                                \n",
      "\n",
      "                                        Close\n",
      "                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ad Feedback\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Close icon\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                  \n",
      "                  SCOTUS\n",
      "                \n",
      "\n",
      "\n",
      "\n",
      "                  \n",
      "                  Congress\n",
      "                \n",
      "\n",
      "\n",
      "\n",
      "                  \n",
      "                  Facts First\n",
      "                \n",
      "\n",
      "\n",
      "\n",
      "                  \n",
      "                  2024 Elections\n",
      "                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                    More\n",
      "                  \n",
      "\n",
      "\n",
      "\n",
      "                    SCOTUS\n",
      "                  \n",
      "\n",
      "                    Congress\n",
      "                  \n",
      "\n",
      "                    Facts First\n",
      "                  \n",
      "\n",
      "                    2024 Elections\n",
      "                  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            Watch\n",
      "          \n",
      "\n",
      "            Listen\n",
      "          \n",
      "\n",
      "            Live TV\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Sign in\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "      My Account\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "          Settings\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "          Topics You Follow\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "          Log Out\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your CNN account\n",
      "Sign in to your CNN account\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Sign in\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "      My Account\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "          Settings\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "          Topics You Follow\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "          Log Out\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your CNN account\n",
      "Sign in to your CNN account\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            Live TV\n",
      "          \n",
      "\n",
      "            Listen\n",
      "          \n",
      "\n",
      "            Watch\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Edition\n",
      "              \n",
      "\n",
      "\n",
      "                  US\n",
      "                \n",
      "\n",
      "\n",
      "\n",
      "                  International\n",
      "                \n",
      "\n",
      "\n",
      "\n",
      "                  Arabic\n",
      "                \n",
      "\n",
      "\n",
      "\n",
      "                  Espa√±ol\n",
      "                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Edition\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                  US\n",
      "                \n",
      "\n",
      "\n",
      "\n",
      "                  International\n",
      "                \n",
      "\n",
      "\n",
      "\n",
      "                  Arabic\n",
      "                \n",
      "\n",
      "\n",
      "\n",
      "                  Espa√±ol\n",
      "                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                  SCOTUS\n",
      "                \n",
      "\n",
      "\n",
      "\n",
      "                  Congress\n",
      "                \n",
      "\n",
      "\n",
      "\n",
      "                  Facts First\n",
      "                \n",
      "\n",
      "\n",
      "\n",
      "                  2024 Elections\n",
      "                \n",
      "\n",
      "\n",
      "\n",
      "Follow CNN Politics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        World\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "            Africa\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Americas\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Asia\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Australia\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            China\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Europe\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            India\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Middle East\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            United Kingdom\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        US Politics\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "            SCOTUS\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Congress\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Facts First\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            2024 Elections\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Business\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "            Tech\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Media\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Calculators\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Videos\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Markets\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "            Pre-markets\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            After-Hours\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Market Movers\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Fear & Greed\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            World Markets\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Investing\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Markets Now\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Before the Bell\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Nightcap\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Health\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "            Life, But Better\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Fitness\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Food\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Sleep\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Mindfulness\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Relationships\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Entertainment\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "            Movies\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Television\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Celebrity\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Tech\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "            Innovate\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Gadget\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Foreseeable Future\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Mission: Ahead\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Upstarts\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Work Transformed\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Innovative Cities\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Style\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "            Arts\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Design\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Fashion\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Architecture\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Luxury\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Beauty\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Video\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Travel\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "            Destinations\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Food & Drink\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Stay\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            News\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Videos\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Sports\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "            Football\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Tennis\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Golf\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Motorsport\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            US Sports\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Olympics\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Climbing\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Esports\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Hockey\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Watch\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "            Live TV\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Digital Studios\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            CNN Films\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            HLN\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            TV Schedule\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            TV Shows A-Z\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            CNNVR\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Features\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "            As Equals\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Call to Earth\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Freedom Project\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Impact Your World\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Inside Africa\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            2 Degrees\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            CNN Heroes\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            All Features\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Weather\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "            Climate\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Wildfire Tracker\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Video\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        More\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "            Photos\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Longform\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Investigations\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            CNN Profiles\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            CNN Leadership\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            CNN Newsletters\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Work for CNN\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ad Feedback\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "      Why does Biden keep mentioning January 6? Because Trump won‚Äôt stop talking about it\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\tBy Edward-Isaac Dovere, Steve Contorno and Annie Grayer, CNN\n",
      "\t\t\n",
      "\n",
      "\n",
      "\n",
      "              10 minute read\n",
      "            \n",
      "\n",
      "  Updated\n",
      "        1:30 PM EDT, Sat March 16, 2024\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            Link Copied!\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Follow:\n",
      "\n",
      "\n",
      "Congressional news\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "See all topics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "President Joe Biden and former President Donald Trump.\n",
      "\n",
      "Reuters/AP\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CNN\n",
      "         — \n",
      "    \n",
      "\n",
      "\n",
      "            The rallies start with a recording of January 6 prisoners singing the national anthem. Campaign staff hand out pre-made ‚ÄúToo Big to Rig‚Äù signs to supporters. When the candidate takes the stage, he calls the rioters ‚Äúpeople who love our country‚Äù and ‚Äúhostages unfairly imprisoned for long periods of time.‚Äù\n",
      "    \n",
      "\n",
      "            There is nothing subtle about how central Donald Trump has made January 6, 2021, to his campaign. More than just continuing to feed denialism and conspiracies about the 2020 election, he is constantly distorting the reality of what happened that day, preaching vindication to his¬†base of voters.\n",
      "    \n",
      "\n",
      "            In ways big and small ‚Äì but often overlooked because they have become so commonplace at his events ‚Äì ¬†the former president¬†glosses over the violence. He promises pardons for the people who committed it.\n",
      "    \n",
      "\n",
      "            On this, Trump and President Joe Biden agree: January 6 itself is a central issue of the 2024 campaign and will be even if Trump‚Äôs trials on related indictments get delayed past Election Day.\n",
      "    \n",
      "\n",
      "            It‚Äôs Biden‚Äôs campaign aides who have been surprised how much that‚Äôs true.\n",
      "    \n",
      "\n",
      "            ‚ÄúPeople know what happened on January 6,‚Äù said Mike Donilon, one of Biden‚Äôs closest advisers. ‚ÄúI think most of the country is going to say, ‚ÄòWe don‚Äôt embrace political violence. We do embrace democracy. We do embrace the rule of law. We‚Äôre not interested in pardoning people who ransacked the Capitol, and we‚Äôre going to have a real problem supporting someone who embraces all that.‚Äô‚Äù\n",
      "    \n",
      "\n",
      "            Though Donilon and a few others ‚Äî including Vice President Kamala Harris, in private conversations to CNN ‚Äî had been adamant for three years¬†that January 6 would continue reverberating, Biden aides use words like ‚Äústunning‚Äù to describe the way Trump has not just kept January 6 present, but burrowed ever deeper into conspiracy theories that are embraced in the right-wing echo chamber but push away more mainstream¬†voters.\n",
      "    \n",
      "\n",
      "            And while Biden¬†aides in the Wilmington reelection office have been closely monitoring¬†Trump‚Äôs¬†rallies, stockpiling clips for future use to likely pair with the many disturbing videos of the mob breaking down the doors and attacking police, they don‚Äôt need to go further than keeping an eye on Trump‚Äôs Truth Social account.\n",
      "    \n",
      "\n",
      "‚ÄúMy first acts as your next President will be to Close the Border, DRILL, BABY, DRILL, and Free the January 6 Hostages being wrongfully imprisoned!‚Äù¬†he wrote.\n",
      "    \n",
      "\n",
      "            Some Biden aides say they were shocked that January 6 keeps coming up in every focus group, to the point that Democratic operatives these days tend to use words like ‚Äúindelible image‚Äù or ‚Äúscar tissue‚Äù to describe how the memories still hit.\n",
      "    \n",
      "\n",
      "            ‚ÄúWe were all surprised,‚Äù acknowledged one senior Democrat involved with the reelection effort, asking not to be named to describe private strategy development.\n",
      "    \n",
      "\n",
      "            ‚ÄúAnyone who is being honest was surprised Jan. 6 continues to be this resonant,‚Äù the person added. ‚ÄúBut in hindsight, when you combine extreme rhetoric, extreme policy and lasting imagery, that ends up being a pretty powerful memory.‚Äù\n",
      "    \n",
      "\n",
      "            The Trump campaign did not respond to questions sent by CNN about his embrace of January 6.\n",
      "    \n",
      "\n",
      "    A running mate litmus test\n",
      "\n",
      "\n",
      "            Biden aides say January 6 and the footage from it will be a central feature of their advertising campaigns, the convention and beyond. They are counting on more attention from Trump‚Äôs trials, if those happen‚Äîand already, a Biden senior aide told CNN they have thought through how the president will keep talking about the topic while insisting he is sticking to not interfering in the legal process.\n",
      "    \n",
      "\n",
      "            Trump, for his part,¬†keeps talking about it and his aides aren‚Äôt really trying to stop him. The rally crowds keep cheering. And every ambitious Republican trying to get in Trump‚Äôs line of sight as he draws out the jockeying to be his running mate knows the bar for entry: Was Mike Pence in the wrong on January 6 ‚Äî and if you were in a similar position in the future, would you be ready to toss out electoral votes in favor of a conspiracy to keep your losing boss in power?\n",
      "    \n",
      "\n",
      "            New York. Rep. Elise Stefanik¬†and Ohio Sen. J.D. Vance¬†have said that they would not have certified the election results in 2020 as constitutionally obligated. Stefanik would not commit to certifying the next election. South Carolina Sen. Tim Scott refused to say what he would have done if¬†he was in Pence‚Äôs position at the time. Trump‚Äôs housing secretary Ben Carson¬†and North Dakota Gov. Doug Burgum wouldn‚Äôt answer¬†when asked¬†if Pence did the right thing.¬†As presidential candidates, Scott and Burgum had both indicated that Pence was in the right during a debate last August.\n",
      "    \n",
      "\n",
      "            Pence has defended his actions many times. On Friday, he said he would not be endorsing Trump.\n",
      "    \n",
      "\n",
      "    Faded hopes that January 6 would be ‚Äòdecisive moment‚Äô\n",
      "\n",
      "\n",
      "            Biden always says the reason he ran at all in 2020 was because he wouldn‚Äôt let America be a place where the 2017 neo-Nazi march in Charlottesville happened or was equivocated about from the White House. He started his presidency not wanting to believe that America would be a place where so many Republicans almost immediately absolved Trump, and where so many of those who didn‚Äôt quickly fell into line anyway.\n",
      "    \n",
      "\n",
      "            Biden‚Äôs speech at the US¬†Capitol on the one-year anniversary of January 6 at first seemed like a kind of capstone to the topic ‚Äî and¬†not as essentially a preview of the speech he would end up giving at Valley Forge on the third anniversary as a way to kick off his 2024 campaign year.\n",
      "    \n",
      "\n",
      "            ‚ÄúThere was a hope that January 6 would have been a decisive moment in terms of the threat to democracy. But it‚Äôs not. It‚Äôs an ongoing battle,‚Äù Donilon said.\n",
      "    \n",
      "\n",
      "            Fewer Trump voters think the protesters who entered the Capitol were ‚Äúmostly violent‚Äù than they did in the past, according to Washington Post-University of Maryland surveys from December of last year and in 2021. Among his supporters, there‚Äôs been a significant jump in the view that punishment is too harsh for those who broke into the Capitol¬†from 45% to 57%, and nearly 9 in 10 of them now think that Trump bears little or no responsibility for the attack.\n",
      "    \n",
      "\n",
      "            The same, though, cannot be said for most other Americans, including the kinds of voters who swung away from Trump in the last election. For example, 61% of college educated White voters still think the protesters who breached the Capitol walls were violent, up slightly from 2021. About 56 percent of independent voters continue to believe Trump was largely responsible for it, virtually unchanged during that stretch.\n",
      "    \n",
      "\n",
      "    Congressional Republicans eager to give Trump more January 6 fodder\n",
      "\n",
      "\n",
      "            In one of his first moves after taking over the job, House¬†Speaker Mike Johnson announced¬†in November he would release all Capitol Hill security footage from January 6, 2021, that does not contain sensitive information.\n",
      "    \n",
      "\n",
      "            GOP Rep. Barry Loudermilk of Georgia is spearheading the Republican-led investigation into the work of the former January 6 select committee, going after star witnesses of the probe and alleging the former select committee withheld witness transcripts from the public to undercut some of their most explosive claims.\n",
      "    \n",
      "\n",
      "            That‚Äôs earned Loudermilk being called a ‚Äúhero‚Äù by Trump at that rally in Georgia.\n",
      "    \n",
      "\n",
      "            Meanwhile, leading Trump ally GOP Rep. Matt Gaetz of Florida introduced a largely symbolic resolution last month declaring Trump did not incite an insurrection or rebellion on January 6. More than one-third of the House Republican Conference have signed on.\n",
      "    \n",
      "\n",
      "            Some are true believers. Some, said GOP Rep. Tim Burchett of Tennessee, are attached because ‚ÄúI‚Äôm sure they are afraid not to.‚Äù\n",
      "    \n",
      "\n",
      "            Some Republicans argue it‚Äôs the Democrats who are keeping the issue alive.\n",
      "    \n",
      "\n",
      "            ‚ÄúIt happened, not the very best day. But Democrats are focusing on it because they have nothing else to focus on,‚Äù GOP Rep. Jeff Van Drew of New Jersey said of January 6. ‚ÄúThey have to make you look at the shiny object over here because otherwise you‚Äôre going to keep your eye on the game. And in the game, [Biden] is losing badly. He‚Äôs in trouble.‚Äù\n",
      "    \n",
      "\n",
      "    Appeals to voters\n",
      "\n",
      "\n",
      "            Biden aides are confident that pardoning acts of political violence isn‚Äôt popular, but also isn‚Äôt anywhere near top of mind for people wondering what a president‚Äôs top three priorities are for Day One. But it‚Äôs more than that: While Trump is talking about what he will do for insurrectionists, Biden aides believe they‚Äôll be able to make the case to voters that the president will make a difference in the lives of people who aren‚Äôt in jail for invading the Capitol.\n",
      "    \n",
      "\n",
      "            Biden aides believe that argument draws in wavering swing voters, while seeing those images of January 6 will help keep anti-Trump Republicans repelled from the former president and lights up the Democratic base to boost turnout.\n",
      "    \n",
      "\n",
      "            Aides know one of the issues they‚Äôre running up against is younger voters and voters of color who have been telling pollsters and focus groups that protecting ‚Äúdemocracy‚Äù doesn‚Äôt mean as much to them because they start out skeptical that democracy was ever working that well. That‚Äôs why several top Democratic strategists working with the Biden campaign, as well as Harris herself, have been stressing ‚Äúfreedom‚Äù instead, believing that gets people more actively connected to abortion rights, LGBTQ rights and other fights.\n",
      "    \n",
      "\n",
      "            But Biden aides say that‚Äôs a big part of why the images and memories of January 6 itself are so important.\n",
      "    \n",
      "\n",
      "            Black voters have responded in focus groups by being offended to see the right to vote that they or their parents and grandparents fought for be abused like that. Those conversations can also quickly turn to dark speculation about how much rougher and bloody the response from law enforcement would have been if the rioters had been Black.\n",
      "    \n",
      "\n",
      "            Latinos who either immigrated themselves or are children of immigrants have told focus groups that they are upset to see people disrespect the process that they worked so hard to take an oath of citizenship to be part of.\n",
      "    \n",
      "\n",
      "            Matt Barreto ‚Äì one of Biden‚Äôs pollsters in 2020 who then continued to conduct focus groups and polls for the Democratic National Committee and remains close to the reelection campaign ‚Äì said that unlike abortion, Ukraine aid, Israel, or pretty much any other issue in a divided time, January 6 stands apart.\n",
      "    \n",
      "\n",
      "            Barreto shared one response from a recent focus group of undecided Latino voters that he oversaw for a Latino advocacy group: ‚Äúthe way they‚Äôre going about it, and the time that is taking them to prosecute the criminal acts that he engaged in is just is very dangerous game they‚Äôre playing with‚Äîbecause a lot of his people are, for you know, a dictatorship. Apparently, they never had it, so they don‚Äôt know what it is, or if they do, they support it. And unfortunately, the U.S. is young, and we never had a dictatorship.‚Äù\n",
      "    \n",
      "\n",
      "            ‚ÄúPeople are going to sit around and talk about the state of the economy, the cost of gas, and other things like that. Those frustrations are real. We know that, and we‚Äôre taking them very seriously,‚Äù Barreto said. ‚ÄúBut when it gets to January 6, it still hits a nerve with people.‚Äù\n",
      "    \n",
      "\n",
      "    Biden not only Democrat counting on January 6 revulsion\n",
      "\n",
      "\n",
      "            Biden isn‚Äôt the only Democrat counting on voters to be repelled by Trump‚Äôs approach to January 6. In Arizona, Democrats are hoping that the combination of Republican candidate Kari Lake‚Äôs 2020 election denialism and her refusal to accept the results of her own 2022 gubernatorial race will make her a more toxic candidate to voters who almost elected her last time.\n",
      "    \n",
      "\n",
      "            Rep. Chris Deluzio, a Democrat running to hold onto his seat in a divided district in top presidential battleground Pennsylvania ‚Äî where one of the local Republican leaders was a fake elector himself who has not been condemned by the congressman‚Äôs Republican challenger ‚Äî said January 6 is ‚Äúremarkably still out there for folks.‚Äù\n",
      "    \n",
      "\n",
      "            ‚ÄúThere are Republicans and independents who are just disgusted that their nominee is the guy who tried to rip up the Constitution,‚Äù Deluzio said. ‚ÄúIrrespective of politics, these aren‚Äôt our values.‚Äù\n",
      "    \n",
      "\n",
      "            Will Rollins, a former federal prosecutor involved in several cases of Southern Californians who were at the Capitol that day and is running in a rematch as a Democrat against GOP Rep. Ken Calvert ‚Äî who voted to overturn the results after the riot and said recently he hopes the people in jail get out ‚Äî said that he hears constantly from independents and Republicans who say that Trump‚Äôs embrace of January 6 is continuing to drive them away.\n",
      "    \n",
      "\n",
      "            ‚ÄúIt‚Äôs become bigger than January 6. Those images of course are visceral for people, and we all remember seeing them on TV,‚Äù Rollins said. ‚ÄúIt‚Äôs more of a forward-looking threat that people recognize. They‚Äôre thinking about not January 6 of 2021, but they‚Äôre thinking about the next certification in 2025.‚Äù\n",
      "    \n",
      "\n",
      "            CNN‚Äôs Ariel Edwards-Levy contributed to this report.\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ad Feedback\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ad Feedback\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ad Feedback\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ad Feedback\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ad Feedback\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ad Feedback\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Sign in\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "      My Account\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "          Settings\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "          Topics You Follow\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "          Log Out\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your CNN account\n",
      "Sign in to your CNN account\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Live TV\n",
      "      \n",
      "\n",
      "        Listen\n",
      "      \n",
      "\n",
      "        Watch\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        World\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "            Africa\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Americas\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Asia\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Australia\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            China\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Europe\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            India\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Middle East\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            United Kingdom\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        US Politics\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "            SCOTUS\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Congress\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Facts First\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            2024 Elections\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Business\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "            Markets\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Tech\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Media\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Calculators\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Videos\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Health\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "            Life, But Better\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Fitness\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Food\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Sleep\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Mindfulness\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Relationships\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Entertainment\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "            Movies\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Television\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Celebrity\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Tech\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "            Innovate\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Gadget\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Foreseeable Future\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Mission: Ahead\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Upstarts\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Work Transformed\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Innovative Cities\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Style\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "            Arts\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Design\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Fashion\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Architecture\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Luxury\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Beauty\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Video\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Travel\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "            Destinations\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Food & Drink\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Stay\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            News\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Videos\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Sports\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "            Football\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Tennis\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Golf\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Motorsport\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            US Sports\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Olympics\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Climbing\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Esports\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Hockey\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Watch\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "            Live TV\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Digital Studios\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            CNN Films\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            HLN\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            TV Schedule\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            TV Shows A-Z\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            CNNVR\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Features\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "            As Equals\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Call to Earth\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Freedom Project\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Impact Your World\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Inside Africa\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            2 Degrees\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            CNN Heroes\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            All Features\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Weather\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "            Climate\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Wildfire Tracker\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Video\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        More\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "            Photos\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Longform\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Investigations\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            CNN Profiles\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            CNN Leadership\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            CNN Newsletters\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            Work for CNN\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            Watch\n",
      "          \n",
      "\n",
      "            Listen\n",
      "          \n",
      "\n",
      "            Live TV\n",
      "          \n",
      "\n",
      "\n",
      "Follow CNN Politics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Sign in\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "      My Account\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "          Settings\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "          Topics You Follow\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "          Log Out\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your CNN account\n",
      "Sign in to your CNN account\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "          Terms of Use\n",
      "        \n",
      "\n",
      "          Privacy Policy\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "          Ad Choices\n",
      "        \n",
      "\n",
      "          Accessibility & CC\n",
      "        \n",
      "\n",
      "          About\n",
      "        \n",
      "\n",
      "          Newsletters\n",
      "        \n",
      "\n",
      "          Transcripts\n",
      "        \n",
      "\n",
      "¬© 2024 Cable News Network. A Warner Bros. Discovery Company. All Rights Reserved.  CNN Sans ‚Ñ¢ & ¬© 2016 Cable News Network.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "# WebPage\n",
    "loader = WebBaseLoader(\"https://edition.cnn.com/2024/03/16/politics/biden-trump-january-6/index.html\")\n",
    "data = loader.load()\n",
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "# pdf\n",
    "loader = PyPDFLoader(\"paper-1-SCL.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Text Splitter 실습하기**\n",
    "- 텍스트를 Chunk 단위로 분할\n",
    "- CharacterTextSplitter: 특정 구분자를 기준으로 텍스트를 정크로 분할, 단, Charactor Split은 Chunf size를 초과하는 경우가 발생하여 대부분 Recursive 모듈을 자주사용\n",
    "- RecursiveCharacterTextSplitter: 일반적인 글로 된 문서는 모두 textsplitter로 분할"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1362, which is longer than the specified 1000\n",
      "Created a chunk of size 1458, which is longer than the specified 1000\n",
      "Created a chunk of size 1170, which is longer than the specified 1000\n",
      "Created a chunk of size 2019, which is longer than the specified 1000\n",
      "Created a chunk of size 1211, which is longer than the specified 1000\n",
      "Created a chunk of size 1212, which is longer than the specified 1000\n",
      "Created a chunk of size 1082, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract—Classification of multisensor signals is an im- portant problem in maintaining stable process operations, particularly in advancing predictive modeling for early de- tection of abnormal states. Self-supervised learning meth- ods, one of the representation learnings, have been widely studied. However, they have focused on using unlabeled data. In this study, we aim to address the challenge of effectively utilizing fully labeled data for modeling multi- sensor signals. We introduce supervised contrastive learn- ing (SCL) for the classification of multisensor signals. Our training framework involves a two-step process: SCL for en- coder pretraining with time-series data augmentations, and classifier training with the pretrained encoder. Our method exhibits superior performance, outperforming traditional supervised learning approaches by a substantial margin. Furthermore, we demonstrate the practical applicability of our approach for early prediction problems through experi- ments conducted with real-process data obtained from au- tomobile engine manufacturing. Our work offers a promis- ing method for multisensor signal analysis and early fault detection in manufacturing industries.\n",
      "Index Terms—Automobile engine manufacturing, classi- fication, multisensor signals, supervised contrastive learn- ing (SCL), time-series data augmentation.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MULTISENSOR signal data are widely encountered in manufacturing systems [1]. Multisensor signal data are composed of time-series values obtained from multiple sensors attached to process equipment. Multiple sensors monitor aspects such as acceleration and vibration of the process states and machinery, producing a flow of signal data that can represent the features of both normal and abnormal states.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Classification of such multisensor signal data is an impor- tant and challenging problem in the manufacturing process. In particular, identifying abnormal signals plays a crucial role in maintaining stable process operations and preventing equipment failures. Traditional approaches, including rule-based systems, statistical methods, and signal-processing techniques, have ad- dressed this problem [2]. However, their implementation neces- sitates domain expertise and prior knowledge for identifying abnormal patterns in diverse sensor types. Moreover, these ap- proaches encounter challenges in effectively capturing complex patterns inherent in extensive quantities of multisensor signal data [3].\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "with open(\"paper-1-SCL_abstract_intro_relatedwork.txt\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\\n\",\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 100,\n",
    "    length_function = len,\n",
    ")\n",
    "texts = text_splitter.split_text(state_of_the_union)\n",
    "print(texts[0])\n",
    "print(\"-\"*100)\n",
    "print(texts[1])\n",
    "print(\"-\"*100)\n",
    "print(texts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "including fault detection and diagnosis and machinery health\n",
      "monitoring [4],[5],[6],[7]. Nevertheless, they make an essential\n",
      "demand for manual feature extraction or dimensionality reduc-\n",
      "tion steps [3]. It is difﬁcult to perform the joint optimization of\n",
      "the feature extraction method and the machine learning model,resulting in the inability of the extracted features to directly\n",
      "represent the abnormal status of equipment [8]. Consequently,\n",
      "exploring an effective method for representation learning ofmultisensor signal data becomes imperative.\n",
      "Deep learning models incorporate representation learning and\n",
      "classiﬁcation tasks in uniﬁed network architecture. Representa-\n",
      "tion learning involves training models to automatically discover\n",
      "and create meaningful representations or features from raw data.Long short-term memory and convolutional neural networks\n",
      "(CNNs) have become the primary methods with their outstand-\n",
      "ing predictive performance in time-series modeling [1],[9],[10].\n",
      "Although the CNNs have been successfully applied to image\n",
      "and text data, they have also shown remarkable performance in\n",
      "time-series data [1]. They have applied for various predictive\n",
      "tasks, including fault detection and diagnosis [11],[12],[13],\n",
      "[14], machinery health monitoring [8],[10], and early detection\n",
      "[15],[16] in many industries.\n",
      "In recent studies, the importance of representation learn-\n",
      "ing as a pretraining task has emerged in performing super-\n",
      "vised classiﬁcation tasks [17],[18]. The main pretraining strat-\n",
      "egy is called self-supervised learning. Self-supervised learning\n",
      "1551-3203 © 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\n",
      "See https://www.ieee.org/publications/rights/index.html for more information.This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \n",
      "Authorized licensed use limited to: Korea University. Downloaded on March 05,2024 at 05:28:56 UTC from IEEE Xplore.  Restrictions apply.\n"
     ]
    }
   ],
   "source": [
    "print(pages[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "including fault detection and diagnosis and machinery health\n",
      "monitoring [4],[5],[6],[7]. Nevertheless, they make an essential\n",
      "demand for manual feature extraction or dimensionality reduc-\n",
      "tion steps [3]. It is difﬁcult to perform the joint optimization of\n",
      "the feature extraction method and the machine learning model,resulting in the inability of the extracted features to directly\n",
      "represent the abnormal status of equipment [8]. Consequently,\n",
      "exploring an effective method for representation learning ofmultisensor signal data becomes imperative.\n",
      "Deep learning models incorporate representation learning and\n",
      "classiﬁcation tasks in uniﬁed network architecture. Representa-\n",
      "tion learning involves training models to automatically discover\n",
      "and create meaningful representations or features from raw data.Long short-term memory and convolutional neural networks\n",
      "(CNNs) have become the primary methods with their outstand-\n",
      "ing predictive performance in time-series modeling [1],[9],[10].\n",
      "Although the CNNs have been successfully applied to image\n",
      "and text data, they have also shown remarkable performance in\n",
      "time-series data [1]. They have applied for various predictive\n",
      "tasks, including fault detection and diagnosis [11],[12],[13],\n",
      "[14], machinery health monitoring [8],[10], and early detection\n",
      "[15],[16] in many industries.\n",
      "In recent studies, the importance of representation learn-\n",
      "ing as a pretraining task has emerged in performing super-\n",
      "vised classiﬁcation tasks [17],[18]. The main pretraining strat-\n",
      "egy is called self-supervised learning. Self-supervised learning\n",
      "1551-3203 © 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\n",
      "See https://www.ieee.org/publications/rights/index.html for more information.This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \n",
      "Authorized licensed use limited to: Korea University. Downloaded on March 05,2024 at 05:28:56 UTC from IEEE Xplore.  Restrictions apply.\n"
     ]
    }
   ],
   "source": [
    "texts = text_splitter.split_documents(pages)\n",
    "print(texts[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "\n",
    "loader = PyPDFLoader(\"paper-2-QDL.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap = 50, length_function = tiktoken_len)\n",
    "texts = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='4374 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS: SYSTEMS, VOL. 52, NO. 7, JULY 2022\\nQuality-Discriminative Localization of Multisensor\\nSignals for Root Cause Analysis\\nYoon Sang Cho and Seoung Bum Kim\\nAbstract —Root cause analysis (RCA) methods for effectively\\nidentifying the critical causes of abnormal processes haveattracted attention because manufacturing processes have grown\\nin scale and complexity. However, the existing methods for\\nbuilding automatic RCA models suffer from the disadvantageof typically requiring expert knowledge. In addition, withouta dataset representing the causal relationship of multivari-ate processes, it is difﬁcult to provide useful information forRCA. Although data-driven RCA methods have been proposed,most are based on classiﬁcation models. Given that productquality is deﬁned as a continuous variable in many manufac-turing industries, classiﬁcation models are limited in derivingroot causes affecting the product quality level. In this article, wepropose a regression model-based RCA method, which we callquality-discriminative localization, consisting of a convolutionalneural network (CNN)-based activation mapping of multisen-sor signal data. In our proposed method, the CNN predicts theproduct quality of a continuous variable. Activation mappingthen extracts causal maps that highlight signiﬁcant sensor sig-nals for each product. To identify the root causes, we generatea root cause map from the weighted sum of quality and causalmaps. We consider root causes as locations of abnormal processesand processing times from localized activation scores on the rootcause map. We experimentally demonstrate the usefulness of theproposed method with simulated data and real process data froma steel manufacturing process. Our results show that the proposedmethod successfully identiﬁes root causes with distinct sensorsignal patterns.\\nIndex Terms —Activation mapping, convolutional neural\\nnetwork (CNN), multisensor signal data, quality-discriminativelocalization, root cause analysis (RCA), steel manufacturing.\\nI. I NTRODUCTION\\nROOT cause analysis (RCA) plays a key role in main-\\ntaining stable manufacturing processes. An RCA aims\\nto determine the critical causes of abnormal processes and\\nManuscript received March 6, 2021; accepted June 28, 2021. Date of', metadata={'source': 'paper-2-QDL.pdf', 'page': 0}),\n",
       " Document(page_content='taining stable manufacturing processes. An RCA aims\\nto determine the critical causes of abnormal processes and\\nManuscript received March 6, 2021; accepted June 28, 2021. Date of\\npublication July 26, 2021; date of current version June 16, 2022. Thiswork was supported in part by the Brain Korea 21 FOUR, Ministry of\\nScience and ICT (MSIT) in Korea through the ITRC Support Program super-\\nvised by the IITP under Grant IITP-2020-0-01749; in part by the NationalResearch Foundation of Korea grant funded by the MSIT under Grant NRF-2019R1A4A1024732; and in part by the Ministry of Culture, Sports and\\nTourism and Korea Creative Content Agency under Grant R2019020067.\\nThis article was recommended by Associate Editor L. Chen. (Corresponding\\nauthor: Seoung Bum Kim.)\\nThe authors are with the School of Industrial and Management\\nEngineering, Korea University, Seoul 02841, Republic of Korea (e-mail:\\nyscho187@korea.ac.kr; sbkim1@korea.ac.kr).\\nColor versions of one or more ﬁgures in this article are available at\\nhttps://doi.org/10.1109/TSMC.2021.3096529.\\nDigital Object Identiﬁer 10.1109/TSMC.2021.3096529product defects [1]. Recently, RCA methods have drawn atten-\\ntion because modern manufacturing processes have becomemore automated and interlinked [2]. Once a process has an\\nabnormal event with an unknown root cause, it adversely\\naffects other processes. In particular, in large-scale and com-plex manufacturing systems, failing to detect root causes\\nleads to recurrent problems, which can, unchecked, engender\\nmachine breakdowns and decrease productivity [3]. Therefore,an RCA model that appropriately explains the relationship\\nbetween process states and product quality is required.\\nThis article addresses an RCA problem for understanding\\nroot causes, considering the following issues. First, abnor-\\nmal signals, such as noisy symptoms, should be detected', metadata={'source': 'paper-2-QDL.pdf', 'page': 0}),\n",
       " Document(page_content='This article addresses an RCA problem for understanding\\nroot causes, considering the following issues. First, abnor-\\nmal signals, such as noisy symptoms, should be detected\\nbecause they are directly associated with abnormal process\\nstates and decreased product quality. Second, the RCA model\\nmust identify multilevel causes because problems occur inrelation to multivariate processes and processing times. Third,\\nthe RCA method must detect not only temporary causes\\nbut also the unknown root causes that intrinsically lead toabnormal processes.\\nWith advanced sensor technology, real-time multisensor sig-\\nnal data can be collected in many industries. Real-worldexamples include human activity recognition [4], machin-\\nery health monitoring [5], and manufacturing processes [6].\\nSensor signals have been used to recognize human activity;the detection of abnormal signals of human activity has been\\nutilized in healthcare areas for providing patient information,\\nsuch as fainting, falling, and headaches. In machinery healthmonitoring, sensor signals can represent the process state of\\nequipment, and the detection of abnormal signals has been\\nconsidered a crucial task for preventing equipment faults. In\\nmanufacturing systems, monitoring abnormal sensor signals,\\nsuch as ﬂuctuations, is necessary for process control in a nor-mal state. Thus, the identiﬁcation and explanation of abnormal\\nsignals are important issues.\\nIn manufacturing systems, such sensor data represent\\nsequential processes and processing time information, which\\ndetermine product quality. When the datasets include the\\ncausal factor, RCA involves two steps: 1) construction ofa predictive model and 2) identiﬁcation of the root cause.\\nThe prediction model explains the relationship between pro-\\ncess states and product quality, and then it detects the rootcause from the sensor data with the highlighted parameters of\\nthe predictive model [1], [7].\\nIn general, RCA models are either probabilistic or determin-\\nistic. In probabilistic model-based RCA, Bayesian networks\\nthat can achieve probabilistic reasoning have been widely\\n2168-2216 c⃝2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\\nSee https://www.ieee.org/publications/rights/index.html for more information.', metadata={'source': 'paper-2-QDL.pdf', 'page': 0}),\n",
       " Document(page_content='2168-2216 c⃝2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\\nSee https://www.ieee.org/publications/rights/index.html for more information.\\nAuthorized licensed use limited to: New York University. Downloaded on January 04,2024 at 23:51:00 UTC from IEEE Xplore.  Restrictions apply.', metadata={'source': 'paper-2-QDL.pdf', 'page': 0}),\n",
       " Document(page_content='CHO AND KIM: QUALITY-DISCRIMINATIVE LOCALIZATION OF MULTISENSOR SIGNALS FOR ROOT CAUSE ANALYSIS 4375\\nused. These Bayesian networks derive the posterior probabil-\\nities and can detect changes in the sensor data [3], [8]–[10].Weidl et al. [3] proposed an object-oriented Bayesian network,\\nwhich is a probabilistic graphical model that performs rea-\\nsoning under uncertainty. Nawaz et al. [8] and Liu et al. [9]\\nconsidered the cause-and-effect relationships between root\\ncauses, equipment, and process parameters using a Bayesian\\nnetwork. Furthermore, Wee et al. [10] proposed a Bayesian\\nbelief network-based causal knowledge model that provides\\ncausal strength using a fuzzy cognitive map. However, the\\nabove-mentioned RCA models are knowledge based anddepend on expert knowledge [11]. Although they are useful\\nfor identifying immediate causes, requiring expert knowledge\\nis disadvantageous in building automatic RCA methods for\\nlarge-scale systems.\\nData-driven RCA methods based on deterministic models\\nhave become more attractive in modern industries because\\nthey can derive root causes from observations without model\\nuncertainty [6]. Chien et al. [7] used Hotelling’s T\\n2for sen-\\nsor variable selection and analyzed the association between\\nfaulty products and sensor variables using decision trees.\\nMahadevan and Shah [1] proposed a one-class support vec-tor machine (SVM) to identify abnormal processes and used\\nSVM recursive feature elimination to determine root causes.\\nSaﬁzadeh and Latiﬁ [12] proposed feature extraction usingprincipal component analysis and data-level, feature-level, and\\ndecision-level fusion of multiple sensors. They then used\\nk-nearest neighbor algorithms to perform classiﬁcation in\\nbearing fault diagnosis. Although these methods perform rea-\\nsonably well within the industrial realms for which they were\\ndesigned, there is still much scope for improvement. Many\\nfeature selection methods suffer from computational complex-\\nity in large-volume datasets. Furthermore, rule-based models,such as decision trees, facilitate the interpretation of results,\\nbut they cannot be readily used with raw sensor signals.\\nRecently, with the increasing popularity of deep learn-\\ning owing to its computational and predictive performance,\\ndeep learning-based RCA models have become prominent in', metadata={'source': 'paper-2-QDL.pdf', 'page': 1}),\n",
       " Document(page_content='but they cannot be readily used with raw sensor signals.\\nRecently, with the increasing popularity of deep learn-\\ning owing to its computational and predictive performance,\\ndeep learning-based RCA models have become prominent in\\nvarious ﬁelds [2], [13]–[15]. Deep neural networks directlyuse multiple sensor signals as the input and automatically\\nlearn the desired information from the input data. Chen and\\nLi [16] proposed sparse autoencoder-based feature extrac-tion for multiple accelerometer sensors and performed clas-\\nsiﬁcation using a deep belief network for bearing fault\\ndiagnosis. Jing et al. [17] used deep convolutional neural\\nnetworks (CNNs) for multiple fault classiﬁcation by construct-\\ning two-dimensional (2-D) multisensor data.\\nCNNs have also been widely used for the diagnosis\\nof defect causes in manufacturing processes. For example,\\nLee et al. [11] proposed a CNN structure, in which a recep-\\ntive ﬁeld tailored to multisensor signals slides along the time\\naxis, to extract fault feature maps providing abnormal process\\nvariables and time information. Azamfar et al. [13] proposed\\nthe multisensor data fusion for gearbox fault diagnosis using\\n2-D CNN for motor current signature analysis. Yang et al. [18]\\nproposed a Spearman rank correlation-based CNN architectureto extract useful features of multiple time-series signals and\\nrecognize fault features’ locations. In addition, Yao et al. [19]attempted fault diagnosis using a CNN and temporal attention\\nmechanism to extract meaningful temporal parts from sensorsignals. Assaf and Schumann [14] also proposed explain-\\nable deep neural networks for multivariate time series. They\\ndesigned a two-stage CNN architecture and used a gradient-based class activation mapping (grad-CAM) for interpreting\\nthe prediction results of average energy production.\\nRecently, the model-agnostic methods, including the local', metadata={'source': 'paper-2-QDL.pdf', 'page': 1}),\n",
       " Document(page_content='the prediction results of average energy production.\\nRecently, the model-agnostic methods, including the local\\ninterpretable model agnostic explanation (LIME) [20] and\\nShapley additive explanation (SHAP) [21], have gained much\\ninterest. They provide local interpretability by quantifyingthe contribution of features for the individual prediction. The\\nLIME perturbs the input data and observes the resulting impact\\nof the prediction. The SHAP assigns each feature impor-\\ntance by exploiting the Shapely values from game theory.\\nSchlegel et al. [22] used LIME and SHAP for time-series\\nexplanation and compared their performances with various\\nmachine learning models. As a similar approach, the coun-\\nterfactual explanation [23] describes a causal situation byexamining how the features should change to obtain a differ-\\nent prediction [24]. Ates et al. [25] used the counterfactual\\nexplanation method to explain multivariate time series andshowed its better performance compared to LIME and SHAP.\\nAll the aforementioned methods were proposed for classiﬁca-\\ntion purposes. Although such classiﬁcation model-based RCAsexhibit good performance, they cannot be applied when con-\\ntinuous values represent the output variable. For example, in\\nsemiconductor manufacturing processes, quality is determinedby defect rates. In steel-making processes, quality can be\\ndetermined by numerical values, such as the weight deviation\\nbetween the target and output products.\\nA few RCA methods to handle regression problems have\\nbeen proposed. Xia et al. [26] used spectral regression for\\nfault feature extraction based on multisensor signal data.\\nBorchert et al. [27] attempted to use a partial least squares\\nregression model that can derive variable importance; theycompared its performances when using raw sensor signals\\nand using features extracted by principal component analy-\\nsis. However, when using high-dimensional sensor signals,such approaches require feature selection or an extraction\\nstep, which are cumbersome for users. In addition, such\\napproaches have difﬁculty deriving local explainability, indi-cating observation-wise causes because they focus on select-\\ning signiﬁcant variables. Granger causality [28] and transfer', metadata={'source': 'paper-2-QDL.pdf', 'page': 1}),\n",
       " Document(page_content='approaches have difﬁculty deriving local explainability, indi-cating observation-wise causes because they focus on select-\\ning signiﬁcant variables. Granger causality [28] and transfer\\nentropy [29] measuring statistical causality between two time-series data can be used for an RCA, but this study addresses\\ntime-series data as an input and one response variable as an\\noutput.\\nSchockaert et al. [15] attempted to derive local and global\\ninterpretability explaining the multivariate time-series data for\\nthe regression problem. They proposed a method combin-\\ning CNN-based guided backpropagation and long short-term\\nmemory (LSTM)-based attention mechanism for a blast fur-nace process in steel manufacturing. The method predicts\\nthe hot metal temperature and derives local and global\\nsaliency maps that visualize temporal and spatial interpretabil-ity. However, the RCA was not performed in a testing process\\nand, thus, its performance cannot be veriﬁed. They only\\nAuthorized licensed use limited to: New York University. Downloaded on January 04,2024 at 23:51:00 UTC from IEEE Xplore.  Restrictions apply.', metadata={'source': 'paper-2-QDL.pdf', 'page': 1}),\n",
       " Document(page_content='4376 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS: SYSTEMS, VOL. 52, NO. 7, JULY 2022\\nshowed the potential of their proposed method without proper\\nvalidations of RCA results. Schockaert et al. [30] used the\\nvariational autoencoder and LIME for local interpretability of\\ndata-driven models that forecast the hot metal temperature for\\nthe blast furnace process in steel manufacturing. However, theyalso missed proper validations to evaluate the RCA results.\\nThis article proposes a CNN architecture-based\\nRCA approach that combines a 2-D CNN model andactivation mapping for deriving causal sensor signals in\\na regression system. Our method can visually explain which\\nparts of the sensor signal cause abnormal quality. Thus,we call the proposed method is a quality-discriminative\\nlocalization. The main contributions of this study can be\\nsummarized as follows.\\n1) Our study presents an RCA approach for making a CNN\\nexplainable for predicting a continuous output with mul-tivariate time-series inputs. The RCA based on class\\nactivation mapping (CAM) has been used for classiﬁ-\\ncation purposes in most of the existing literature, butour study presents CAM for regression problems.\\n2) Our study presents the detailed and proper valida-\\ntion procedure in both simulation and real data todemonstrate the usefulness and applicability of the\\nproposed method.\\nThe remainder of this article is organized as follows.\\nSection II introduces the discriminative localization technique\\nusing a CNN. Section III describes the details of the proposed\\nmethod. Section IV presents a simulation study to examinethe performance of the proposed method and compare it with\\nother methods under different scenarios. Section V presents\\na case study to demonstrate the applicability of the proposed\\nmethod using real data from a steel manufacturing process.\\nFinally, Section VI presents our concluding remarks.\\nII. D\\nISCRIMINATIVE LOCALIZATION\\nWhile CNNs have been widely used in visual recognition\\nproblems [31], a number of previous works have proposed\\nvisualizing the causality of CNN predictions by highlightingpixels that play an important role in prediction [32]–[34]. The\\nmost relevant to our study is the CAM approach to class-\\ndiscriminative localization. Zhou et al. [32] proposed a CAM\\nmethod for identifying discriminative regions using image', metadata={'source': 'paper-2-QDL.pdf', 'page': 2}),\n",
       " Document(page_content='most relevant to our study is the CAM approach to class-\\ndiscriminative localization. Zhou et al. [32] proposed a CAM\\nmethod for identifying discriminative regions using image\\nclassiﬁcation with restricted classes. They replaced fully con-nected layers with convolutional layers and a global average\\npooling (GAP) layer to produce class-speciﬁc feature maps.\\nThe GAP layer performs a downsampling operation, and itcalculates the average values of each feature map as follows:\\nF\\nk=1\\n(W·H)·∑\\nx,yfk(x,y) (1)\\nwhere fk(x, y)i st h e kth feature map, ( x, y) represents the\\nx-axis and y-axis information of the feature map, and Wand\\nHrepresent the width and height of the feature maps, respec-\\ntively. This yields the global average value Fk. A GAP layer\\nis typically added between a prediction network and a fea-ture extraction network. Instead of adding a fully connected\\nlayer for ﬂattened features, the summarized vector F\\nkis feddirectly into the activation function. One advantage of a GAP\\nlayer over a fully connected layer is that it makes it easier toconstruct an interpretable CNN by enforcing correspondences\\nbetween feature maps and labels.\\nAfter training the CNN, the CAM derives an activation map\\nby a weighted combination of the resulting feature values\\nof the GAP and weights of a soft-max activation function.\\nThe activation map then indicates which parts of an inputimage were considered by the CNN for assigning labels. The\\nactivation map M\\nc(x,y) for class cis derived as follows:\\nMc(x,y)=K∑\\nk=1fk(x,y)·wc\\nk (2)\\nwhere fk(x,y) and wkcare the kth feature map and learned\\nweights of the last dense layer, respectively. The CAM con-\\nsiders wkcas the importance of fk(x,y). In this study, we built\\na regression version of the CNN-based activation mapping that', metadata={'source': 'paper-2-QDL.pdf', 'page': 2}),\n",
       " Document(page_content='weights of the last dense layer, respectively. The CAM con-\\nsiders wkcas the importance of fk(x,y). In this study, we built\\na regression version of the CNN-based activation mapping that\\ncan be used to predict the quality label of a continuous vari-able and to highlight the critical regions of 2-D multisensor\\nsignal data.\\nIII. Q\\nUALITY -DISCRIMINATIVE LOCALIZATION\\nOur goal is to derive visually explainable root causes\\nfrom multisensor signal data. In this section, we present thearchitecture of the CNN, followed by the proposed quality-\\ndiscriminative localization method.\\nFig. 1 presents an overview of the quality-discriminative\\nlocalization using a CNN. The CNN consists of a feature\\nextraction network and a prediction network [31]. The feature\\nextraction network aims to construct feature maps that con-\\ntain distinctive features of input data. As the CNN’s essential\\ncomponents, the convolutional layers extract local features ofthe input, and then the output is typically fed into a downsam-\\npling layer, such as a max-pooling layer, to achieve translation\\ninvariance over small spatial shifts in the input data [35].These layers form a convolution block and generate feature\\nmaps according to the number of blocks. It is noteworthy\\nthat because we aim to localize the root cause location ofmultisensor signals in a pixelwise manner, the ﬁnal feature\\nmaps must be formed with the same size as that of the input\\ndata. Thus, we must add an upsampling operation for reducedfeature maps.\\nIn this study, to investigate the performance of the quality-\\ndiscriminative localization in various feature extraction archi-tectures, we adopted several structures that have been widely\\nused in the image segmentation ﬁeld because they simply and\\neffectively construct ﬁnal feature maps of the same size asthe input. We detail the adopted structures and compare their\\nperformances in Section IV .\\nThe prediction network comprises a 2-D GAP layer and\\na dense layer. The GAP layer averages the ﬁnal convolutional\\nfeature map f\\nk(t,s), and the resulting values Fkare fed into\\nthe dense layer. The dense layer has one neuron with a lin-\\near activation function that computes a linear combination of', metadata={'source': 'paper-2-QDL.pdf', 'page': 2}),\n",
       " Document(page_content='feature map f\\nk(t,s), and the resulting values Fkare fed into\\nthe dense layer. The dense layer has one neuron with a lin-\\near activation function that computes a linear combination of\\nﬂattened feature values and learnable weight parameters [31].The linear activation function directly passes the input value\\nand then enables the CNN to perform a regression. Thus,\\nAuthorized licensed use limited to: New York University. Downloaded on January 04,2024 at 23:51:00 UTC from IEEE Xplore.  Restrictions apply.', metadata={'source': 'paper-2-QDL.pdf', 'page': 2}),\n",
       " Document(page_content='CHO AND KIM: QUALITY-DISCRIMINATIVE LOCALIZATION OF MULTISENSOR SIGNALS FOR ROOT CAUSE ANALYSIS 4377\\nFig. 1. Overview of the proposed CNN-based quality-discriminative localization.\\nthe formulation of the CNN regression can be summarized\\nas follows:\\nY=W·F=w1·F1+w2·F2+···+ wk·Fk (3)\\nwhere Yis a response variable, described by the averaged fea-\\nture map Fof extracted feature maps and weight parameter W.\\nNote that Wcan be considered as the coefﬁcients that enable\\nus to identify the signiﬁcant variables in the regression model.\\nAfter training the model, we use the trained weight W∗as the\\nimportance for each Fkwhen deriving the activation map.\\nWe train the CNN model such that the following cost\\nfunction L(mean squared error) is minimized:\\nL=1\\nNN∑\\nn=1(\\nyn−ˆyn)2(4)\\nwhere Nis the number of observations, ynis the nth response\\nvariable, and ˆynis the predicted value, which is the output of\\nthe CNN. The CNN is trained using an Adam optimizer, whichis an algorithm for the ﬁrst-order gradient-based stochastic\\nobjective functions.\\nWe now present the proposed quality-discriminative local-\\nization for RCA. The purpose of the proposed method is to\\nvisually emphasize the sensor signals that represent the rootcauses of abnormal quality. As shown in Fig. 1, we ﬁrst derive\\ncausal maps with activation mapping for all products, and then\\nwe identify the root causes with the localized sensor signalsin the RCM. Having trained the CNN, we conduct the activa-\\ntion mapping to produce the following causal maps from theweighted sum of | w\\nk∗| and fk(t, s):\\nCM n(t,s)=K∑\\nk=1fn\\nk(t,s)·⏐⏐w∗\\nk⏐⏐ (5)\\nwhere | wk∗| is the absolute value of trained weights wk∗,\\nwhich can be considered the importance of each featuremap f', metadata={'source': 'paper-2-QDL.pdf', 'page': 3}),\n",
       " Document(page_content='k⏐⏐ (5)\\nwhere | wk∗| is the absolute value of trained weights wk∗,\\nwhich can be considered the importance of each featuremap f\\nk(t, s)[ 3 6 ] .T h e CM n(t, s) score highlights the impor-\\ntant regions of the input data corresponding to the label.\\nConsidering that the regression model determines importantvariables based on the magnitude of weights, we use the abso-\\nlute value of w\\nk∗.CM n(t, s) explains the causes of a product’s\\npredicted quality by localizing sensor signals. If a region con-tains high CM\\nn(t, s) scores, the corresponding region’s sensor\\nsignals can be considered signiﬁcant causes.\\nIn the present study, we aim to identify the root causes,\\nwhose processes and processing times indicate an increase in\\nweight deviation. Thus, we obtain a quality-weighted activa-\\ntion map, called the RCM. By computing the weighted sumof weight deviation and causal maps, the RCM represents\\nthe most critical causes of abnormal quality. The RCM is\\ncalculated as follows:\\nRCM(t,s)= N∑\\nnCM n(t,s)·ˆyn (6)\\nwhere CM n(t,s) is an activated map of the nth obser-\\nvation, and ˆynis a predicted value. Having constructed\\nthe RCM, we examine the localized positions that have\\nscores exceeding percentile h. Thus, we interpret that the\\nAuthorized licensed use limited to: New York University. Downloaded on January 04,2024 at 23:51:00 UTC from IEEE Xplore.  Restrictions apply.', metadata={'source': 'paper-2-QDL.pdf', 'page': 3}),\n",
       " Document(page_content='4378 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS: SYSTEMS, VOL. 52, NO. 7, JULY 2022\\nTABLE I\\nLOCATIONS OF ROOT CAUSES\\nAlgorithm 1 Quality-Discriminative Localization\\n1: Input: Data Xandy;\\n2: Output: RCM∗(t,s)\\n3:⊿Train the model\\n4: w←Initialize the parameters\\n5: Repeat\\n6: ˆy←Model (X)=Fk·wk\\n7: L=1\\nN·∑N\\nn=1(\\nyn−ˆyn)2\\n8: w←Update the parameters using the gradients of L\\n9: until convergence of the parameters using L\\n10:⊿Construct the RCM\\n11: RCM (t,s)←Initialize elements of the tbysmatrix to zero\\n12: for n=1 to N do\\n13: CMn(t,s)←∑K\\nk=1fn\\nk(t,s)·|w∗\\nk|\\n14: RCM (t,s)←RCM (t,s)+CMn(t,s)·ˆyn\\n15: end for\\n16:⊿Identify the root causes\\n17: Let hbe a hyperparameter that indicates a threshold as a percentile.\\n18: IfRCM (t,s)>hthen RCM (t,s)=root cause’s location\\n19: RCM∗(t,s)←root cause’s location\\nlocalized sensor signal is an indicator of the cause of ˆyn.\\nThe causal maps show the causes of defects in the prod-ucts, and the RCM explains the root causes accounting for\\nall product quality. Algorithm 1 shows the procedure of\\nquality-discriminative localization.\\nIV . S\\nIMULATION STUDY\\nWe evaluated the quality-discriminative localization on sim-\\nulated datasets in terms of its predictive performance and\\nlocalization ability.\\nA. Simulated Data\\nWe generated nine datasets composed of 1000 observations,\\nwhich consist of multiple sensor signals of 30 ×100 size,\\nrepresenting the process states of a manufacturing system with30 processes and 100 processing times. We ﬁrst sampled the\\nsensor signals from a normal distribution with mean 0 and', metadata={'source': 'paper-2-QDL.pdf', 'page': 4}),\n",
       " Document(page_content='representing the process states of a manufacturing system with30 processes and 100 processing times. We ﬁrst sampled the\\nsensor signals from a normal distribution with mean 0 and\\nstandard deviation 0.1, representing the normal operation of\\nprocesses. We also sampled the response variable, indicatinga manufacturing system’s defect rates by sampling 1000 values\\nfrom a normal distribution with mean 60 and standard devi-\\nation 20. We then generated abnormal signals of three causetypes by sampling values from a sine wave according to the\\ndefect rates. The cause types (A), (B), and (C) were generated\\nfrom defect rates ×sine wave of causal time length, defect\\nrates+sine wave of causal time length, and defect rates ×\\nsine wave (from – πtoπ) of causal time length, respectively,\\nwhich represent many ﬂuctuations, a broad-scale peak, anda large-scale ﬂuctuation, as shown in Fig. 2.\\nWe then associated them with root cause locations. Table I\\nlists the root cause locations for each dataset under differ-ent scenarios: 1) consecutive causal processes have the same\\ncausal processing times (see Datasets 1, 2, and 3); 2) non-\\nconsecutive causal processes have different causal process-\\ning times (see Datasets 4, 5, and 6); and 3) all of the\\ncausal processes have different causal processing times (seeDatasets 7, 8, and 9). They indicate that causal processes and\\nprocessing times can occur from different abnormal signal pat-\\nterns and multiple causal locations in a manufacturing system.Using these datasets, we examined the predictive performance\\nand localization ability.\\nWe conducted experiments with ﬁve CNN architectures\\nto investigate the quality-discriminative localization abil-\\nity of various feature extraction architectures, including\\na fully convolutional network (FCN), U-Net, a deconvolutionalnetwork (DeconvNet), and SegNet. These are popular CNN\\narchitectures for image segmentation that perform pixelwise\\nclassiﬁcation and can extract feature maps of the same size\\nAuthorized licensed use limited to: New York University. Downloaded on January 04,2024 at 23:51:00 UTC from IEEE Xplore.  Restrictions apply.', metadata={'source': 'paper-2-QDL.pdf', 'page': 4}),\n",
       " Document(page_content='CHO AND KIM: QUALITY-DISCRIMINATIVE LOCALIZATION OF MULTISENSOR SIGNALS FOR ROOT CAUSE ANALYSIS 4379\\nFig. 2. Examples of three types of abnormal signals with 30 causal time lengths and 66 defect rates. Each type of abnormal signal is generated from (a) 66 ×sine\\nwave of length 30, (b) 66 +sine wave of length 30, and (c) 66 ×sine wave (from – πtoπ) of length 30.\\nFig. 3. Structures of feature extraction networks.\\nas the input data. Fig. 3 shows the structures of these feature\\nextraction networks.\\nThe FCN consists of a convolutional network classiﬁer\\nfollowed by an upsampling layer, performing bilinear inter-\\npolation [37]. We used VGGNet and DenseNet for theFCN [38], [39]. VGGNet has convolution blocks, each con-\\ntaining two convolution layers and a max-pooling layer.\\nDenseNet is composed of dense blocks, performing concate-nation of the previous feature maps and subsequent feature\\nmaps. A dense block also has two convolution layers, and\\neach convolution layer is fed into a batch normalization (BN)layer with a rectiﬁed linear unit (ReLU) activation function.\\nA dense block is followed by a transition layer consisting of\\na BN layer and a 1 ×1 convolutional layer, followed by an\\naverage pooling layer.\\nU-Net has a U-shaped network with a contracting path and\\nan expansive path. The contracting path consists of convolution\\nblocks, where each block has two convolutional and BN layers,\\nfollowed by a max-pooling layer. The expansive path consistsof feature map upsampling, followed by convolution blocks.\\nThe upsampled feature maps are then concatenated with the\\ncorresponding feature map of the convolution blocks in thecontracting path [40].\\nDeconvNet has an encoder-decoder structure, consisting\\nof convolution blocks and deconvolution blocks, respec-tively, [41]. The deconvolution operation is identical to the\\nconvolution operation but is hierarchically opposite. Each\\nblock has two consecutive convolution operations, followed bya BN layer with the ReLU activation function. Before a decon-', metadata={'source': 'paper-2-QDL.pdf', 'page': 5}),\n",
       " Document(page_content='convolution operation but is hierarchically opposite. Each\\nblock has two consecutive convolution operations, followed bya BN layer with the ReLU activation function. Before a decon-\\nvolution block, two fully connected layers are added, and thenan unpooling layer, which restores the max-pool indices in\\nevery step of the decoder’s convolution operation, is applied.\\nAs with DeconvNet, SegNet has an encoder–decoder struc-\\nture, but it uses only convolution blocks in both the encoder\\nand decoder and does not use any fully connected layers [42].\\nWe employed a smaller version of each model to consider\\nthe size of the simulated data and to minimize the difference\\nin the numbers of parameters between models. We used only\\none convolution block in the FCNs. We also used one convo-lution block for each downsampling and upsampling operation\\nin U-Net, DeconvNet, and SegNet. Each convolutional layer\\nwas set to generate 32 feature maps with 2-D convolutional\\nkernels of 3 ×3 size. Moreover, we used 2-D ﬁlters of 2 ×2\\nsize for the downsampling and upsampling layers. Then, toenable them to perform the regression model-based RCA, we\\nreshaped their prediction networks to consist of a GAP layer\\nand a dense layer with a linear activation function.\\nB. Evaluation of the Predictive Performance\\nWe performed ﬁvefold cross-validation using 80% and 20%\\nas the training and testing data, respectively. We trained\\nthe CNN with a batch size of eight and epoch size of\\n100 for all datasets. We also implemented an early stop-ping criterion, where if the validation loss did not improve\\nafter 30 epochs, the training would terminate. All models\\nAuthorized licensed use limited to: New York University. Downloaded on January 04,2024 at 23:51:00 UTC from IEEE Xplore.  Restrictions apply.', metadata={'source': 'paper-2-QDL.pdf', 'page': 5}),\n",
       " Document(page_content='4380 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS: SYSTEMS, VOL. 52, NO. 7, JULY 2022\\nTABLE II\\nAVERAGE PREDICTIVE PERFORMANCES IN FIVEFOLD CROSS -VALIDATIONS .STANDARD DEVIATIONS AREPRESENTED IN PARENTHESES\\nwere programmed in Python using Keras and a TensorFlow\\n2.0 backend. We conducted all experiments on a workstationequipped with an Intel Core I9-9900 CPU @ 3.10 GHz, 64-GB\\nDDR4 RAM 3200 MHz, NVIDIA GeForce RTX 2080 Ti\\nwith 4352 CUDA cores, and Windows 10 Enterprise operating\\nsystem.\\nWe used performance measures of R\\n2, mean absolute\\nerror (MAE), and root mean squared error (RMSE) as follows:\\nR2=∑N\\nn=1(\\nˆyn−y)2\\n∑N\\nn=1(yn−y)2(7)\\nMAE =1\\nNN∑\\nn=1⏐⏐yn−ˆyn⏐⏐ (8)\\nRMSE =\\ued6a\\ued6b\\ued6b√1\\nNN∑\\nn=1(\\nyn−ˆyn)2(9)\\nwhere ynandˆynare the actual and predicted values of the\\nnth observation, respectively. R2represents the coefﬁcient\\nof determination calculated by the square of the correlationbetween yandˆy. MAE is the average value over the val-\\nidation data of the absolute differences between the actual\\nand predicted values, where all individual differences haveequal weights. RMSE is the square root of the mean squared\\ndifference between the actual and predicted values for the\\nvalidation data.\\nTable II lists the averages and standard deviations of the\\nﬁvefold cross-validation results. The average R\\n2, MAE, and\\nRMSE values of all datasets are listed in the last row.\\nAlthough the FCN-DenseNet network showed the most accu-\\nrate performance, with average R2, MAE, and RMSE values\\nof 0.98, 1.81, and 2.35, respectively, the performances of', metadata={'source': 'paper-2-QDL.pdf', 'page': 6}),\n",
       " Document(page_content='rate performance, with average R2, MAE, and RMSE values\\nof 0.98, 1.81, and 2.35, respectively, the performances of\\nthe other methods are comparable. We can conclude that our\\nsimulated sensor data explain a response variable well undervarious scenarios, and this opens the possibility of performing\\nRCA based on such models.TABLE III\\nAUC S CORES OF THE QUALITY -DISCRIMINATIVE LOCALIZATION\\nC. Evaluation of the Localization Ability\\nIn the simulation study, the speciﬁc threshold h, deter-\\nmining how many causal pixels would be localized, has\\nnot been set because we compare the localization abil-\\nities of CNN architectures for all ranges of threshold-ing values based on the area under the curve (AUC).\\nThe AUC indicates the area under the receiver operat-\\ning characteristic (ROC) curve. The ROC curve is cre-ated by plotting the true-positive rate (TPR) against the\\nfalse-positive rate (FPR) under various threshold settings.\\nThe TPR is the rate of correctly predicted locations ofroot causes based on a speciﬁc threshold h, and FPR\\nhas the opposite meaning. The AUC can be deﬁned\\nas follows:\\nAUC=∫\\n1\\nh=0TPR(\\nFPR−1(h))\\ndh (10)\\nwhere the his the threshold of quality-weighted\\nactivation score in an RCM; we generate causal\\nmaps and RCMs using ﬁvefold test data for\\nAuthorized licensed use limited to: New York University. Downloaded on January 04,2024 at 23:51:00 UTC from IEEE Xplore.  Restrictions apply.', metadata={'source': 'paper-2-QDL.pdf', 'page': 6}),\n",
       " Document(page_content='CHO AND KIM: QUALITY-DISCRIMINATIVE LOCALIZATION OF MULTISENSOR SIGNALS FOR ROOT CAUSE ANALYSIS 4381\\nFig. 4. Illustration of a causal maps: the red regions indicate more signiﬁcant regions than blue regions, and the yellow bounding boxes indicate regi ons of\\nabnormal signals.\\nFig. 5. Illustration of RCMs generated from the SegNet-based quality-discriminative localization.\\neach dataset, and we report the AUC scores in\\nTable III.\\nThe SegNet architecture outperformed the other methods,\\nand we found that the encoder–decoder structure, performingmore convolution operations during upsampling with max-\\npool indices, shows higher accuracy in root cause localization.\\nFig. 4 jointly illustrates the multisensor signals and a causalmap, highlighting abnormal signals of the highest defect rate\\nproduct in the Dataset 3 in which the SegNet architecture\\nshowed the most accurate AUC score. Fig. 5 illustrates RCMsgenerated from the quality-discriminative localization for alldatasets, highlighting the root cause regions. We can conﬁrm\\nthat the proposed method correctly emphasizes causal regions.\\nV. C\\nASE STUDY\\nA. Steel Manufacturing Process Data\\nTo evaluate the applicability of the proposed method in\\nthe real world, we conducted experiments on nine datasetscollected from a steel manufacturing process. The steel man-\\nufacturing process produces steel from iron ore and scrap.\\nAuthorized licensed use limited to: New York University. Downloaded on January 04,2024 at 23:51:00 UTC from IEEE Xplore.  Restrictions apply.', metadata={'source': 'paper-2-QDL.pdf', 'page': 7}),\n",
       " Document(page_content='4382 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS: SYSTEMS, VOL. 52, NO. 7, JULY 2022\\nFig. 6. Illustration of data collection from the continuous casting in the steel manufacturing process.\\nTABLE IV\\nCONFIGURATION OF SENSOR VARIABLES\\nOFCONTINUOUS CASTING PROCESS\\nThe entire process is divided into four subprocesses: 1) steel-\\nmaking; 2) reﬁning; 3) continuous casting; and 4) forming.\\nSteelmaking involves the input of raw materials that are melted\\nin a blast furnace. Reﬁning reduces the impurities that canmake the resulting molten steel brittle. Next, continuous cast-\\ning places the molten steel into a cooled mold, causing it\\nto solidify into a thin steel shell. It is then made into anintermediate-stage product, such as a billet, bloom, or slab.\\nThe steel is formed into various shapes, often by hot rolling,\\neliminating cast defects, and achieving the required shape.\\nIn this study, we focused on RCA for the continuous cast-\\ning process, which plays an important role in determining the\\nquality of steel. Fig. 6 illustrates a procedure for collectingmultisensor signal datasets in continuous casting, consisting\\nof the cooling, rolling, and cutting processes. Once the molten\\nsteel enters the continuous casting process, the cooling pro-cess decreases its temperature using mold’s cooling water. It\\nis then cooled and rolled into the shape of an intermediate-\\nstage product, billet, bloom, and slab, using cooling spraysand rollers in the rolling process. In the cutting process, the\\nmolten steel is cut to the desired length using a torch cutting\\nmachine. We measure the difference between the intermediate-\\nstage product’s weight and its target weight after the cutting\\nprocess. This study deﬁnes this weight deviation as indicat-ing the ﬁnal steel product’s quality, enabling us to address\\na regression problem.\\nWe collected nine datasets from multiple sensors attached to\\neach piece of processing equipment. Table IV shows the sen-\\nsor types according to the procedure of the continuous castingTABLE V\\nSUMMARY OF THE 2-D M ULTISENSOR SIGNAL DATASETS\\nprocess: 1) cooling process sensors attached to the rectangular', metadata={'source': 'paper-2-QDL.pdf', 'page': 8}),\n",
       " Document(page_content='sor types according to the procedure of the continuous castingTABLE V\\nSUMMARY OF THE 2-D M ULTISENSOR SIGNAL DATASETS\\nprocess: 1) cooling process sensors attached to the rectangular\\nmold’s four axes (east, west, south, and north), and the sen-\\nsors measure the temperature, pressure, inﬂow rate of cooling\\nwater and mold’s oscillation; 2) rolling process sensors mea-sure the pressure and speed of the roller, values of cooling\\nwater sprays and cooling air sprays; and 3) cutting process\\nsensors measure the length and speed of torch cutting machineduring process operation.\\nUsing these sensors, we obtained sensor data of three prod-\\nuct types, billet, bloom, and slap, having different widths eachother, to verify the robustness and applicability of the proposed\\nmethod under various process states. Table V shows a sum-\\nmary of the datasets. We then transformed the multiple sensorsignals into 2-D data. As shown in Fig. 7, we ﬁrst collect sen-\\nsor data for all processes, and then we sampled the time steps\\nof 100 according to the process sequence to form the CNN’sinput data.\\nConsequently, each product had a 2-D dataset consisting\\nof the process ID on the y-axis and processing time on the\\nx-axis. The response variable is the weight deviation, mea-\\nsured after the cutting process, and its range is −25 to 120,\\nas shown in Fig. 8. The larger the value of the response\\nvariable (weight deviation), the lower the quality. Using\\nthese datasets, we ﬁrst evaluate the predictive performanceand then examine the ability of quality-discriminative\\nlocalization.\\nAuthorized licensed use limited to: New York University. Downloaded on January 04,2024 at 23:51:00 UTC from IEEE Xplore.  Restrictions apply.', metadata={'source': 'paper-2-QDL.pdf', 'page': 8}),\n",
       " Document(page_content='CHO AND KIM: QUALITY-DISCRIMINATIVE LOCALIZATION OF MULTISENSOR SIGNALS FOR ROOT CAUSE ANALYSIS 4383\\nFig. 7. Illustration of the transformation of multisensor signals to 2-D input data.\\nFig. 8. Histograms of response variables (weight deviations) for each dataset.\\nB. Evaluation of the Predictive Performance\\nWe evaluated the predictive performance of SegNet-\\nbased CNN, containing three convolution blocks for eachdownsampling and upsampling operation. We performed ﬁve-\\nfold cross-validation using 80% and 20% as the training\\nand testing data, respectively. We also trained the CNN witha batch size of 8 and epoch size of 100 for all datasets and\\nimplemented an early stopping criterion, where if the valida-\\ntion loss did not improve after 30 epochs, the training would\\nterminate.\\nTable VI presents the averages and standard deviations of\\nthe results of ﬁvefold cross-validation. The average R\\n2,M A E ,\\nand RMSE values of all datasets are listed in the last row.\\nThe average R2, MAE, and RMSE were 0.77, 5.80, and 7.82,\\nrespectively, indicating that the SegNet structure is sufﬁciently\\nrobust to explain the relationship between the sensor signals\\nand product quality in various processes states.\\nC. Evaluation of the Localization Ability\\nWith the regression-trained CNN, we performed quality-\\ndiscriminative localization for RCA. We ﬁrst extracted causalTABLE VI\\nAVERAGE PREDICTIVE PERFORMANCES OF SEGNET+GAP IN\\nFIVEFOLD CROSS -VALIDATIONS .STANDARD DEVIATIONS ARE\\nPRESENTED IN PARENTHESES\\nmaps, which can provide temporary causes for each product,\\nand then we generated an RCM to identify the root causes.\\nFig. 9 shows two examples of causal maps, where the (left)causal map has the lowest weight deviation, and the (right)\\ncausal map has the highest weight deviation on Dataset 1.\\nThe red highlighted regions exhibit more critical factors in thecausal maps than the blue regions in predicting weight devi-\\nation. We considered that the red highlighted sensor signals', metadata={'source': 'paper-2-QDL.pdf', 'page': 9}),\n",
       " Document(page_content='causal map has the highest weight deviation on Dataset 1.\\nThe red highlighted regions exhibit more critical factors in thecausal maps than the blue regions in predicting weight devi-\\nation. We considered that the red highlighted sensor signals\\nrepresent the causal process states of predicted quality; thus,these regions enabled us to derive the location of abnormal\\nsensor signals for the process ID and processing time.\\nAs shown in Fig. 10, we generated RCMs based on the\\nweighted sum of quality and causal maps and derived three-\\ndimensional (3-D) activation maps for each dataset. In each\\nRCM, the x-,y-, and z-axes indicate the process ID, processing\\ntime, and quality-weighted activation score, respectively. Min-\\nmax normalization was performed to represent the quality-\\nweighted activation scores between 0 and 1. We arbitrarily set\\nthe threshold has 90th percentile to derive the most signiﬁcant\\nsensor signals. It depends on the number of causal factorsthat need to be analyzed. If the threshold his set at a lower\\npercentile, more root cause sensors and processing time are\\ndetected. Our empirical results indicate that 90th or 95th workswell in most cases. The red highlighted regions of the RCMs\\nin Fig. 10 indicate major causes of abnormal quality.\\nAuthorized licensed use limited to: New York University. Downloaded on January 04,2024 at 23:51:00 UTC from IEEE Xplore.  Restrictions apply.', metadata={'source': 'paper-2-QDL.pdf', 'page': 9}),\n",
       " Document(page_content='4384 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS: SYSTEMS, VOL. 52, NO. 7, JULY 2022\\nFig. 9. Causal maps: (left) causal map of a steel product with the lowest weight deviation and (right) causal map of a steel product with the highest weig ht\\ndeviation. The red regions indicate more important regions for predicting weight deviations.\\nFig. 10. Results of the RCMs. The red plane indicates the 90th percentile threshold of quality-weighted activation scores.\\nTo statistically verify that the localized regions of process\\nID and processing time exhibit signiﬁcant causal sensor sig-\\nnals, we conducted a two-sample t-test to compare the sensor\\nsignals. We grouped the 100 signals into normal and abnor-\\nmal classes, with 50 signals in each class. We summarized\\neach sensor signal using eight statistical categories: 1) mean;2) variance; 3) min; 4) max; 5) median; 6) range; 7) area;\\nand 8) correlation coefﬁcient. The area represents the sum-\\nmation of a signal for all time steps. We expected that ifeach class had distinctive signal patterns, such statistics could\\nrepresent the characteristics of the sensor signals. We also\\nconducted a correlation analysis to derive a correlation coefﬁ-cient for considering the overall time information. Correlationanalysis was performed between different sensor signals, from\\nwhich we obtained a correlation coefﬁcient matrix. We used\\nan upper triangular matrix, excluding diagonal elements, as the\\ninput to the two-sample t-test and expected that the correlation\\ncoefﬁcients of the sensor signals in the same class would be\\nhigher than those from different classes.\\nUsing these summarized statistics, we performed\\na two-sample t-test to conﬁrm the differences between\\nthe normal and abnormal groups. The tstatistic was\\ncalculated using the following equation:\\nt=Xnormal−Xabnormal√(\\nS_normal∧2)\\n/N_normal −(\\nS_abnormal∧2)\\n/N_abnormal(11)', metadata={'source': 'paper-2-QDL.pdf', 'page': 10}),\n",
       " Document(page_content='calculated using the following equation:\\nt=Xnormal−Xabnormal√(\\nS_normal∧2)\\n/N_normal −(\\nS_abnormal∧2)\\n/N_abnormal(11)\\nAuthorized licensed use limited to: New York University. Downloaded on January 04,2024 at 23:51:00 UTC from IEEE Xplore.  Restrictions apply.', metadata={'source': 'paper-2-QDL.pdf', 'page': 10}),\n",
       " Document(page_content='CHO AND KIM: QUALITY-DISCRIMINATIVE LOCALIZATION OF MULTISENSOR SIGNALS FOR ROOT CAUSE ANALYSIS 4385\\nTABLE VII\\np-VALUES OF THE TWO-SAMPLE t-TEST FOR THE SIGNIFICANT SENSORS ;THOSE IN BOLDFACE ARELESSTHAN 0.05\\nTABLE VIII\\nPREDICTIVE PERFORMANCES OF SEGNET+GAP FOR THE LOCALIZED\\nREGIONS LEARNED BY THE RCM\\nwhere Nnormal and Nabnormal are the sample sizes, S2normal\\nand S2abnormal are the sample variances, and Xnormal and\\nXabnormal indicate the average values of the statistics from the\\nsensor signals. We assumed that the statistics do not have equalvariance.\\nTable VII shows the resulting p-values of the two-sample\\nt-tests. In each dataset, we listed three sensors that ren-\\ndered high activation scores. The p-values, which are less\\nthan 0.05, are highlighted in bold. In some cases, we couldnot obtain the p-values because of the inﬂated tstatis-\\ntics caused by zero variance (indicated by the hyphens in\\nTable VII). Most sensors exhibit p-values less than 0.05,\\nimplying that the selected sensors have distinct features of the\\nsignals. However, we observed some cases with nonsigniﬁcantp-values (e.g., process 149 of dataset 1, process 181 of\\ndataset 7, and process 125 of dataset 8). We believe that these\\ncases cannot be sufﬁciently explained based on the statistics\\nused in this study. Nevertheless, the results showed that most\\nlocalized signals exhibited signiﬁcant differences.\\nWe also evaluated the CNN’s predictive performance using\\nthe multisensor signals located at the localized regions in\\nthe RCM. We selected 50 sensors with the highest quality-weighted activation scores for each dataset, and then we\\nconducted ﬁvefold cross-validation for each selected dataset.\\nWe found that the average R\\n2, MAE, and RMSE were 0.73,\\n6.25, and 8.41, respectively, (Table VIII). These results are\\ncomparable with the predictive CNN results using all regions', metadata={'source': 'paper-2-QDL.pdf', 'page': 11}),\n",
       " Document(page_content='2, MAE, and RMSE were 0.73,\\n6.25, and 8.41, respectively, (Table VIII). These results are\\ncomparable with the predictive CNN results using all regions\\n(see Table VI), indicating that the localized regions learned bythe RCM are signiﬁcant for characterizing a root cause.\\nVI. C\\nONCLUSION\\nWe proposed an RCA method, called quality-discriminative\\nlocalization. With multisensor signal data, a regression-trainedCNN provides productwise causal maps and generates an\\nRCM to identify the most signiﬁcant causal regions in mul-\\ntivariate processes. The causal maps can be used for mon-\\nitoring the causal processes and processing times for all\\nproducts. Then, the RCM can be applied for identifyingthe root causes as processes that frequently cause abnor-\\nmal quality. Consequently, the proposed method enables us\\nto interpret multisensor signals by highlighting distinctivepatterns. In addition, using simulated and real-world pro-\\ncess sensor datasets, we demonstrated that the proposed\\nAuthorized licensed use limited to: New York University. Downloaded on January 04,2024 at 23:51:00 UTC from IEEE Xplore.  Restrictions apply.', metadata={'source': 'paper-2-QDL.pdf', 'page': 11}),\n",
       " Document(page_content='4386 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS: SYSTEMS, VOL. 52, NO. 7, JULY 2022\\nmethod exhibits applicability and robustness with satisfactory\\npredictive performance and localization ability.\\nAlthough the proposed method shows promising results, the\\nactivation mapping using the output of a GAP layer has a lim-\\nitation in that it leads to information loss because the GAPlayer summarizes the ﬁnal feature maps. Nevertheless, the\\nCNN learns the parameters for prediction, even when using the\\nsummarized values of the GAP, and verifying the predictiveperformance before conducting the activation mapping is\\nessential. The proposed CNN also has certain limitations posed\\nby the 2-D kernels in the convolution process. The 2-D kernelextracts a pixelwise element of the local input. When we per-\\nform root cause identiﬁcation, the pixelwise activation scores\\ninvolve risk because each pixel element can contain peripheral\\ninformation from the previous input; in other words, the local-\\nized regions may represent not only the location of the targetpixel but also the surrounding locations. However, because\\nthe interaction relationship is the main factor yielding power-\\nful performance in the CNN, we expect that we can minimizethis concern if we can show good predictive performance using\\na 1-D kernel.\\nUsing the discriminative localization via activation mapping\\nwith the proposed method for identifying optimal processes is\\nan interesting direction for future work. Despite the abnor-\\nmal sensor signals, the detection of optimal sensor signalsthat explain the normal quality can be used to identify the\\nconditions of stable processes. We believe that the proposed\\nmethod can be a cornerstone for the use of activation mappingin multiple sensor data-based root cause monitoring and is\\na useful tool for various manufacturing industries that require\\nmultilevel causal analysis to examine the characteristics of\\nsequential processes.\\nA\\nCKNOWLEDGMENT\\nThe authors would like to thank the editor and reviewers for\\ntheir useful comments and suggestions, which greatly helpedin improving the quality of this article.\\nR\\nEFERENCES\\n[1] S. Mahadevan and S. L. Shah, “Fault detection and diagnosis in pro-\\ncess data using one-class support vector machines,” J. Process Control ,', metadata={'source': 'paper-2-QDL.pdf', 'page': 12}),\n",
       " Document(page_content='R\\nEFERENCES\\n[1] S. Mahadevan and S. L. Shah, “Fault detection and diagnosis in pro-\\ncess data using one-class support vector machines,” J. Process Control ,\\nvol. 19, no. 10, pp. 1627–1639, 2009.\\n[2] F. Jia, Y . Lei, L. Guo, J. Lin, and S. Xing, “A neural network con-\\nstructed by deep learning technique and its application to intelligentfault diagnosis of machines,” Neurocomputing , vol. 272, pp. 619–628,\\nJan. 2018.\\n[3] G. Weidl, A. L. Madsen, and S. Israelson, “Applications of object-\\noriented Bayesian networks for condition monitoring, root cause analysis\\nand decision support on operation of complex continuous processes,”\\nComput. Chem. Eng. , vol. 29, no. 9, pp. 1996–2009, 2005.\\n[4] C. A. Ronao and S.-B. Cho, “Human activity recognition with smart-\\nphone sensors using deep learning neural networks,” Expert Syst. Appl. ,\\nvol. 59, pp. 235–244, Oct. 2016.\\n[5] Y . Lei, N. Li, L. Guo, N. Li, T. Yan, and J. Lin, “Machinery health prog-\\nnostics: A systematic review from data acquisition to RUL prediction,”\\nMech. Syst. Signal Process. , vol. 104, pp. 799–834, May 2018.\\n[6] G. Li, S. J. Qin, and T. Yuan, “Data-driven root cause diagnosis of faults\\nin process industries,” Chemom. Intell. Lab. Syst. , vol. 159, pp. 1–11,\\nDec. 2016.\\n[7] C.-F. Chien, C.-Y . Hsu, and P.-N. Chen, “Semiconductor fault detection\\nand classiﬁcation for yield enhancement and manufacturing intelli-', metadata={'source': 'paper-2-QDL.pdf', 'page': 12}),\n",
       " Document(page_content='[7] C.-F. Chien, C.-Y . Hsu, and P.-N. Chen, “Semiconductor fault detection\\nand classiﬁcation for yield enhancement and manufacturing intelli-\\ngence,” Flex. Serv. Manuf. J. , vol. 25, no. 3, pp. 367–388, 2013.[8] J. M. Nawaz, M. Z. Arshad, and S. J. Hong, “Fault diagnosis in semicon-\\nductor etch equipment using Bayesian networks,” J. Semicond. Technol.\\nSci., vol. 14, no. 2, pp. 252–261, 2014.\\n[9] Z. Liu, Y . Liu, B. Cai, and C. Zheng, “An approach for developing\\ndiagnostic Bayesian network based on operation procedures,” Expert\\nSyst. Appl. , vol. 42, no. 4, pp. 1917–1926, 2015.', metadata={'source': 'paper-2-QDL.pdf', 'page': 12}),\n",
       " Document(page_content='[9] Z. Liu, Y . Liu, B. Cai, and C. Zheng, “An approach for developing\\ndiagnostic Bayesian network based on operation procedures,” Expert\\nSyst. Appl. , vol. 42, no. 4, pp. 1917–1926, 2015.\\n[10] Y . Y . Wee, W. P. Cheah, S. C. Tan, and K. Wee, “A method for root\\ncause analysis with a Bayesian belief network and fuzzy cognitive map,”\\nExpert Syst. Appl. , vol. 42, no. 1, pp. 468–487, 2015.\\n[11] K. B. Lee, S. Cheon, and C. O. Kim, “A convolutional neural network\\nfor fault classiﬁcation and diagnosis in semiconductor manufacturing\\nprocesses,” IEEE Trans. Semicond. Manuf. , vol. 30, no. 2, pp. 135–142,\\nMay 2017.\\n[12] M. S. Saﬁzadeh and S. K. Latiﬁ, “Using multi-sensor data fusion for\\nvibration fault diagnosis of rolling element bearings by accelerometer\\nand load cell,” Inf. Fusion , vol. 18, pp. 1–8, Jul. 2014.\\n[13] M. Azamfar, J. Singh, I. Bravo-Imaz, and J. Lee, “Multisensor data\\nfusion for gearbox fault diagnosis using 2-D convolutional neuralnetwork and motor current signature analysis,” Mech. Syst. Signal\\nProcess. , vol. 144, Oct. 2020, Art. no. 106861.\\n[14] R. Assaf and A. Schumann, “Explainable deep neural networks for mul-\\ntivariate time series predictions,” in Proc. 28th Int. Joint Conf. Artif.\\nIntell. (IJCAI) , 2019, pp. 6488–6490.\\n[15] C. Schockaert, R. Leperlier, and A. Moawad, “Attention mechanism for', metadata={'source': 'paper-2-QDL.pdf', 'page': 12}),\n",
       " Document(page_content='Intell. (IJCAI) , 2019, pp. 6488–6490.\\n[15] C. Schockaert, R. Leperlier, and A. Moawad, “Attention mechanism for\\nmultivariate time series recurrent model interpretability applied to theironmaking industry,” 2020. [Online]. Available: arXiv:2007.12617.\\n[16] Z. Chen and W. Li, “Multisensor feature fusion for bearing fault diag-\\nnosis using sparse autoencoder and deep belief network,” IEEE Trans.\\nInstrum. Meas. , vol. 66, no. 7, pp. 1693–1702, Jul. 2017.\\n[17] L. Jing, T. Wang, M. Zhao, and P. Wang, “An adaptive multi-sensor\\ndata fusion method based on deep convolutional neural networks for\\nfault diagnosis of planetary gearbox,” Sensors , vol. 17, no. 2, p. 414,\\n2017.\\n[18] D. Yang, Y . Pang, B. Zhou, and K. Li, “Fault diagnosis for\\nenergy Internet using correlation processing-based convolutional neu-\\nral networks,” IEEE Trans. Syst., Man, Cybern., Syst. , vol. 49, no. 8,\\npp. 1739–1748, Aug. 2019.\\n[19] Y . Yao, S. Zhang, S. Yang, and G. Gui, “Learning attention represen-\\ntation with a multi-scale CNN for gear fault diagnosis under different\\nworking conditions,” Sensors , vol. 20, no. 4, p. 1233, 2020.\\n[20] S. Lundberg and S.-I. Lee, “A uniﬁed approach to interpreting model\\npredictions,” 2017. [Online]. Available: arXiv:1705.07874.\\n[21] M. T. Ribeiro, S. Singh, and C. Guestrin, “‘Why should i trust you?’\\nExplaining the predictions of any classiﬁer,” in Proc. 22nd ACM', metadata={'source': 'paper-2-QDL.pdf', 'page': 12}),\n",
       " Document(page_content='Explaining the predictions of any classiﬁer,” in Proc. 22nd ACM\\nSIGKDD Int. Conf. Knowl. Discov. Data Min. , 2016, pp. 1135–1144.\\n[22] U. Schlegel, H. Arnout, M. El-Assady, D. Oelke, and D. A. Keim,\\n“Towards a rigorous evaluation of XAI methods on time series,” in\\nProc. IEEE/CVF Int. Conf. Comput. Vis. Workshop (ICCVW) , Seoul,\\nSouth Korea, 2019, pp. 4197–4201, doi: 10.1109/ICCVW.2019.00516 .\\n[23] S. Wachter, B. Mittelstadt, and C. Russell, “Counterfactual explanations\\nwithout opening the black box: Automated decisions and the GDPR,”Harvard J. Law Technol. , vol. 31, no. 2, pp. 841–887, 2018.\\n[24] S. Verma, J. Dickerson, and K. Hines, “Counterfactual explanations\\nfor machine learning: A review,” 2020, [Online]. Available: arXiv:\\n2010.10596.\\n[25] E. Ates, B. Aksar, V . J. Leung, and A. K. Coskun, “Counterfactual expla-\\nnations for machine learning on multivariate time series data,” 2020.\\n[Online]. Available: arXiv: 2008.10781.\\n[26] Z. Xia, S. Xia, L. Wan, and S. Cai, “Spectral regression based fault\\nfeature extraction for bearing accelerometer sensor signals,” Sensors ,\\nvol. 12, no. 10, pp. 13694–13719, 2012.\\n[27] D. Borchert, D. A. Suarez-Zuluaga, P. Sagmeister, Y . E. Thomassen,\\nand C. Herwig, “Comparison of data science workﬂows for root causeanalysis of bioprocesses,” Bioprocess Biosyst. Eng. , vol. 42, no. 2,', metadata={'source': 'paper-2-QDL.pdf', 'page': 12}),\n",
       " Document(page_content='and C. Herwig, “Comparison of data science workﬂows for root causeanalysis of bioprocesses,” Bioprocess Biosyst. Eng. , vol. 42, no. 2,\\npp. 245–256, 2019.\\n[28] H.-S. Chen, Z. Yan, Y . Yao, T.-B. Huang, and Y .-S. Wong, “Systematic', metadata={'source': 'paper-2-QDL.pdf', 'page': 12}),\n",
       " Document(page_content='pp. 245–256, 2019.\\n[28] H.-S. Chen, Z. Yan, Y . Yao, T.-B. Huang, and Y .-S. Wong, “Systematic\\nprocedure for granger-causality-based root cause diagnosis of chemicalprocess faults,” Ind. Eng. Chem. Res. , vol. 57, no. 29, pp. 9500–9512,\\n2018, doi: 10.1021/acs.iecr.8b00697 .\\n[29] L. Ma, J. Dong, and K. Peng, “Root cause diagnosis of quality-related\\nfaults in industrial multimode processes using robust Gaussian mix-ture model and transfer entropy,” Neurocomputing , vol. 285, pp. 60–73,\\nApr. 2018, doi: 10.1016/j.neucom.2018.01.028 .\\n[30] C. Schockaert, V . Macher, and A. Schmitz, “V AE-LIME: Deep genera-\\ntive model based approach for local data-Driven model interpretabil-ity applied to the ironmaking industry,” 2020. [Online]. Available:\\narXiv:2007.10256.\\nAuthorized licensed use limited to: New York University. Downloaded on January 04,2024 at 23:51:00 UTC from IEEE Xplore.  Restrictions apply.', metadata={'source': 'paper-2-QDL.pdf', 'page': 12}),\n",
       " Document(page_content='CHO AND KIM: QUALITY-DISCRIMINATIVE LOCALIZATION OF MULTISENSOR SIGNALS FOR ROOT CAUSE ANALYSIS 4387\\n[31] J. Gu et al. , “Recent advances in convolutional neural networks,” Pattern\\nRecognit. , vol. 77, pp. 354–377, May 2018.\\n[32] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba,\\n“Learning deep features for discriminative localization,” in Proc. IEEE\\nConf. Comput. Vis. Pattern Recognit. , Las Vegas, NV , USA, 2016,\\npp. 2921–2929.\\n[33] K. Muhammad, J. Ahmad, Z. Lv, P. Bellavista, P. Yang, and S. W. Baik,\\n“Efﬁcient deep CNN-based ﬁre detection and localization in video\\nsurveillance applications,” IEEE Trans. Syst. Man, Cybern. Syst. , vol. 49,\\nno. 7, pp. 1419–1434, Jul. 2019.\\n[34] X. Tao, D. Zhang, Z. Wang, X. Liu, H. Zhang, and D. Xu, “Detection of\\npower line insulator defects using aerial images analyzed with convolu-\\ntional neural networks,” IEEE Trans. Syst., Man, Cybern., Syst. , vol. 50,\\nno. 4, pp. 1486–1498, Apr. 2020.\\n[35] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner, “Gradient-based learn-\\ning applied to document recognition,” Proc. IEEE , vol. 86, no. 11,\\npp. 2278–2324, Nov. 1998.\\n[36] R. Tibshirani, “Regression shrinkage and selection via the lasso,”\\nJ. Roy. Stat. Soc. B, Methodol. , vol. 58, no. 1, pp. 267–288, 1996,\\ndoi: 10.1111/j.2517-6161.1996.tb02080.x .', metadata={'source': 'paper-2-QDL.pdf', 'page': 13}),\n",
       " Document(page_content='doi: 10.1111/j.2517-6161.1996.tb02080.x .\\n[37] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks\\nfor semantic segmentation,” in Proc. IEEE Conf. Comput. Vis. Pattern\\nRecognit. , Boston, MA, USA, 2015, pp. 3431–3440.\\n[38] K. Simonyan and A. Zisserman, “Very deep convolutional networks\\nfor large-scale image recognition,” 2014. [Online]. Available: arXiv:1409.1556.\\n[39] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely\\nconnected convolutional networks,” in Proc. IEEE Conf. Comput. Vis.\\nPattern Recognit. , Honolulu, HI, USA, 2017, pp. 4700–4708.\\n[40] O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional\\nnetworks for biomedical image segmentation,” in Proc. Int. Conf. Med.\\nImage Comput. Comput. Assist. Intervention , 2015, pp. 234–241.\\n[41] H. Noh, S. Hong, and B. Han, “Learning deconvolution network for\\nsemantic segmentation,” in Proc. IEEE Int. Conf. Comput. Vis. , Santiago,\\nChile, 2015, pp. 1520–1528.\\n[42] V . Badrinarayanan, A. Kendall, and R. Cipolla, “SegNet: A deep con-\\nvolutional encoder-decoder architecture for image segmentation,” IEEE\\nTrans. Pattern Anal. Mach. Intell. , vol. 39, no. 12, pp. 2481–2495,\\nDec. 2017.\\nYoon Sang Cho received the B.S. degree\\nin industrial and management engineering fromthe Department of Industrial and ManagementEngineering, Hankuk University of Foreign Studies,\\nSeoul, South Korea, in 2017. He is currently pur-\\nsuing the Ph.D. in industrial and management engi-neering degree with the Department of Industrial andManagement Engineering, Korea University, Seoul,\\nRepublic of Korea.', metadata={'source': 'paper-2-QDL.pdf', 'page': 13}),\n",
       " Document(page_content='suing the Ph.D. in industrial and management engi-neering degree with the Department of Industrial andManagement Engineering, Korea University, Seoul,\\nRepublic of Korea.\\nHis research interests include explainable artiﬁcial\\nintelligence algorithms for sequential patterns.\\nSeoung Bum Kim received the M.S. degree in\\nindustrial and systems engineering and the Ph.D.degree in industrial and systems engineering fromthe Georgia Institute of Technology, Atlanta, GA,\\nUSA, in 2001 and 2005, respectively.\\nHe is a Professor with the Department of\\nIndustrial and Management Engineering, KoreaUniversity, Seoul, Republic of Korea. His research\\ninterests utilize machine learning algorithms to cre-\\nate new methods for various problems appearing inengineering and science.\\nAuthorized licensed use limited to: New York University. Downloaded on January 04,2024 at 23:51:00 UTC from IEEE Xplore.  Restrictions apply.', metadata={'source': 'paper-2-QDL.pdf', 'page': 13})]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Text Embedding 실습하기**\n",
    "\n",
    "- 텍스트 문서를 수치 변환하고 질문과 유사도를 비교한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536 1536\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "embeddings = embeddings_model.embed_documents(\n",
    "    [\n",
    "        \"안녕하세요\",\n",
    "        \"제 이름은 조윤상입니다.\",\n",
    "        \"조윤상은 산업경영공학을 전공했습니다\",\n",
    "        \"언어모델은 대용량 텍스트데이터를 학습한 생성형 딥러닝 모델입니다\",\n",
    "        \"Hello World!\",\n",
    "        \n",
    "    ]\n",
    ")\n",
    "len(embeddings), len(embeddings[0])\n",
    "embedded_query_q = embeddings_model.embed_query(\"이 대화에서 언급된 이름은 무엇입니까?\")\n",
    "embedded_query_a = embeddings_model.embed_query(\"이 대화에서 언급된 이름은 조윤상입니다.\")\n",
    "print(len(embedded_query_q), len(embedded_query_a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8899458597020108\n",
      "0.8158938956610856\n",
      "0.8315212592497677\n",
      "0.7884215414829497\n",
      "0.8002830634820086\n",
      "0.7278030214139422\n"
     ]
    }
   ],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "\n",
    "def cos_sim(A, B):\n",
    "    return dot(A, B)/(norm(A)*norm(B))\n",
    "\n",
    "print(cos_sim(embedded_query_q, embedded_query_a))\n",
    "print(cos_sim(embedded_query_q, embeddings[0]))\n",
    "print(cos_sim(embedded_query_q, embeddings[1]))\n",
    "print(cos_sim(embedded_query_q, embeddings[2]))\n",
    "print(cos_sim(embedded_query_q, embeddings[3]))\n",
    "print(cos_sim(embedded_query_q, embeddings[4]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8494187267618187\n",
      "0.8037215308622446\n",
      "0.7287845465894771\n",
      "0.7491667804945571\n",
      "0.7872170923464111\n",
      "0.7816865711989947\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"jhgan/ko-sbert-nli\" # korean\n",
    "model_name = \"BAAI/bge-small-en\" # englilsh\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name = model_name,\n",
    "    model_kwargs = model_kwargs,\n",
    "    encode_kwargs = encode_kwargs,\n",
    ")\n",
    "documents = [\n",
    "    [\n",
    "        \"my name is YoonSangCho\",\n",
    "        \"I majored in Industrial & Management Engineering\",\n",
    "        \"AI is powerful\",\n",
    "        \"where are you from?\",\n",
    "        \"Hello World\",\n",
    "    ]\n",
    "]\n",
    "embeddings = hf.embed_documents(\n",
    "    [\n",
    "        \"my name is YoonSangCho\",\n",
    "        \"I majored in Industrial & Management Engineering\",\n",
    "        \"AI is powerful\",\n",
    "        \"where are you from?\",\n",
    "        \"Hello World\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "BGE_query_q = hf.embed_query(\"Hello? who is this?\")\n",
    "BGE_query_a = hf.embed_query(\"Hi this is YoonSangCho\")\n",
    "\n",
    "print(cos_sim(BGE_query_q, BGE_query_a))\n",
    "print(cos_sim(BGE_query_q, embeddings[0]))\n",
    "print(cos_sim(BGE_query_q, embeddings[1]))\n",
    "print(cos_sim(BGE_query_q, embeddings[2]))\n",
    "print(cos_sim(BGE_query_q, embeddings[3]))\n",
    "print(cos_sim(BGE_query_q, embeddings[4]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.05165728181600571,\n",
       " -0.014959916472434998,\n",
       " 0.017631476745009422,\n",
       " -0.041672028601169586,\n",
       " 0.034519705921411514,\n",
       " -0.0357469841837883,\n",
       " 0.08076842129230499,\n",
       " 0.02722739614546299,\n",
       " 0.02002798207104206,\n",
       " 0.010667991824448109,\n",
       " 0.032536379992961884,\n",
       " -0.06471751630306244,\n",
       " -0.008188118226826191,\n",
       " 0.03445412963628769,\n",
       " 0.007697741966694593,\n",
       " -0.029890544712543488,\n",
       " 0.051178399473428726,\n",
       " -0.016601573675870895,\n",
       " -0.010365455411374569,\n",
       " 0.013720612972974777,\n",
       " 0.008365671150386333,\n",
       " 0.030398201197385788,\n",
       " -0.058151889592409134,\n",
       " -0.049041878432035446,\n",
       " 0.025917071849107742,\n",
       " -0.010762125253677368,\n",
       " -0.004442320205271244,\n",
       " 0.005020192824304104,\n",
       " 0.006352833937853575,\n",
       " -0.1459188163280487,\n",
       " -0.024220235645771027,\n",
       " -0.02214004099369049,\n",
       " 0.013377167284488678,\n",
       " -0.0013176710344851017,\n",
       " 0.005801036953926086,\n",
       " -0.027996528893709183,\n",
       " 0.007844232022762299,\n",
       " -0.0014156651450321078,\n",
       " -0.04077493026852608,\n",
       " -0.011248945258557796,\n",
       " 0.0103797297924757,\n",
       " -0.007679881062358618,\n",
       " 0.01887989602982998,\n",
       " -0.013210931792855263,\n",
       " 0.007122277747839689,\n",
       " -0.03624887764453888,\n",
       " -0.002928373171016574,\n",
       " 0.008817420341074467,\n",
       " -0.0032698565628379583,\n",
       " -0.022593138739466667,\n",
       " 0.009574729017913342,\n",
       " -0.01687830314040184,\n",
       " -0.012852882966399193,\n",
       " -0.019129645079374313,\n",
       " 0.014734098687767982,\n",
       " 0.04484347999095917,\n",
       " 0.06960263103246689,\n",
       " -0.013378073461353779,\n",
       " 0.022165529429912567,\n",
       " 0.02458616904914379,\n",
       " 0.025581592693924904,\n",
       " 0.013295345939695835,\n",
       " -0.1504306197166443,\n",
       " 0.07521913945674896,\n",
       " -0.009778163395822048,\n",
       " 0.03100520186126232,\n",
       " 0.024762991815805435,\n",
       " -0.06123880669474602,\n",
       " 0.025481093674898148,\n",
       " -0.007220091298222542,\n",
       " 0.022328419610857964,\n",
       " 0.0032684095203876495,\n",
       " -0.026045316830277443,\n",
       " -0.003988537937402725,\n",
       " 0.004398169927299023,\n",
       " -0.028119849041104317,\n",
       " 0.04847703501582146,\n",
       " 0.02909979596734047,\n",
       " 0.04741493612527847,\n",
       " 0.0069417161867022514,\n",
       " -0.02752082608640194,\n",
       " -0.03122743032872677,\n",
       " -0.004437299445271492,\n",
       " -0.027755143120884895,\n",
       " -0.028054870665073395,\n",
       " -0.02548361010849476,\n",
       " 0.03198184445500374,\n",
       " 0.02195519022643566,\n",
       " 0.04572173207998276,\n",
       " 0.01693381555378437,\n",
       " -0.04199790954589844,\n",
       " 0.002794074360281229,\n",
       " 0.025351718068122864,\n",
       " -0.00633830763399601,\n",
       " -0.06451574712991714,\n",
       " -0.02292666770517826,\n",
       " 0.01294540986418724,\n",
       " 0.0025044421199709177,\n",
       " -0.09809053689241409,\n",
       " 0.5512164235115051,\n",
       " -0.02623872086405754,\n",
       " 0.012543970718979836,\n",
       " 0.048282165080308914,\n",
       " -0.039926476776599884,\n",
       " 0.010304518975317478,\n",
       " -0.001400829409249127,\n",
       " -0.0013155116466805339,\n",
       " -0.04220161214470863,\n",
       " -0.01975436881184578,\n",
       " -0.03469226881861687,\n",
       " -0.004422908648848534,\n",
       " -0.025570498779416084,\n",
       " 0.012106555514037609,\n",
       " -0.04523010179400444,\n",
       " 0.03505605459213257,\n",
       " -0.03704309090971947,\n",
       " 0.00763260293751955,\n",
       " 0.02683788724243641,\n",
       " 0.0072679403237998486,\n",
       " -0.01665707677602768,\n",
       " 0.03330156207084656,\n",
       " -0.004744306206703186,\n",
       " 0.026955898851156235,\n",
       " 0.023601708933711052,\n",
       " 0.04462679475545883,\n",
       " -0.025799883529543877,\n",
       " 0.0309122446924448,\n",
       " 0.04814115911722183,\n",
       " 0.009973830543458462,\n",
       " 0.03564115986227989,\n",
       " 0.07896099984645844,\n",
       " 0.03745272755622864,\n",
       " -0.08757493644952774,\n",
       " -0.010390431620180607,\n",
       " -0.015429425053298473,\n",
       " 0.025158148258924484,\n",
       " -0.010975562036037445,\n",
       " -0.019279539585113525,\n",
       " -0.009980475530028343,\n",
       " -0.05747172608971596,\n",
       " -0.027089092880487442,\n",
       " -0.035363517701625824,\n",
       " -0.017697647213935852,\n",
       " -0.058713752776384354,\n",
       " -0.050525467842817307,\n",
       " -0.016818942502141,\n",
       " -0.06368983536958694,\n",
       " -0.0003945890348404646,\n",
       " -0.018481876701116562,\n",
       " 0.016024986281991005,\n",
       " -0.025165606290102005,\n",
       " -0.013099338859319687,\n",
       " -0.05750345066189766,\n",
       " -0.03711405396461487,\n",
       " 0.052673302590847015,\n",
       " 0.016117915511131287,\n",
       " 0.06877773255109787,\n",
       " 0.030590252950787544,\n",
       " -0.04557044804096222,\n",
       " 0.02955169416964054,\n",
       " 0.010168344713747501,\n",
       " -0.05066687613725662,\n",
       " -0.0215198565274477,\n",
       " 0.06852679699659348,\n",
       " -0.017060277983546257,\n",
       " -0.148685023188591,\n",
       " -0.011330450884997845,\n",
       " -0.0021099834702908993,\n",
       " -6.132728594820946e-05,\n",
       " -0.0018847135361284018,\n",
       " 0.04250767454504967,\n",
       " 0.023843223229050636,\n",
       " -0.031098537147045135,\n",
       " 0.029745671898126602,\n",
       " 0.05616040527820587,\n",
       " 0.021135156974196434,\n",
       " 0.006184958852827549,\n",
       " 0.017250096425414085,\n",
       " 0.012428044341504574,\n",
       " -0.02646084502339363,\n",
       " 0.0029132179915905,\n",
       " -0.03348289430141449,\n",
       " -0.04036340489983559,\n",
       " -0.000906144268810749,\n",
       " 0.030024908483028412,\n",
       " -0.03437725827097893,\n",
       " 0.004973530303686857,\n",
       " 0.01791277341544628,\n",
       " -0.02445049025118351,\n",
       " 0.01179648656398058,\n",
       " -0.023480286821722984,\n",
       " 0.03153854236006737,\n",
       " -0.014693085104227066,\n",
       " 0.0008082975982688367,\n",
       " -0.02520950697362423,\n",
       " 0.01386419590562582,\n",
       " 0.03338988125324249,\n",
       " -0.012078465893864632,\n",
       " 0.01641201600432396,\n",
       " 0.005869767628610134,\n",
       " 0.013608144596219063,\n",
       " 0.046280864626169205,\n",
       " 0.020948046818375587,\n",
       " 0.05970427766442299,\n",
       " -0.01479122880846262,\n",
       " 0.02432890608906746,\n",
       " 0.0063437409698963165,\n",
       " -0.031115727499127388,\n",
       " 0.05811380222439766,\n",
       " 0.003255808260291815,\n",
       " -0.007461659610271454,\n",
       " 0.01306529063731432,\n",
       " -0.034121233969926834,\n",
       " -0.0037970782723277807,\n",
       " -0.02094944193959236,\n",
       " -0.03562786430120468,\n",
       " -0.009336527436971664,\n",
       " 0.016456978395581245,\n",
       " -0.012040629051625729,\n",
       " 0.04704460874199867,\n",
       " 0.0039031170308589935,\n",
       " -0.05185045301914215,\n",
       " -0.06408598273992538,\n",
       " -0.24503371119499207,\n",
       " 0.01937434822320938,\n",
       " -0.006940928753465414,\n",
       " -0.021377502009272575,\n",
       " 0.006292685866355896,\n",
       " -0.02829546481370926,\n",
       " 0.06270766258239746,\n",
       " -0.01289401575922966,\n",
       " 0.06491585820913315,\n",
       " 0.03821291774511337,\n",
       " 0.03186934068799019,\n",
       " -0.020219868049025536,\n",
       " 0.001494093332439661,\n",
       " 0.027834830805659294,\n",
       " -0.005821876227855682,\n",
       " 0.022172925993800163,\n",
       " 0.026254715397953987,\n",
       " -0.030747490003705025,\n",
       " 0.04444009065628052,\n",
       " -0.0008345699752680957,\n",
       " 0.03641905635595322,\n",
       " -0.011071599088609219,\n",
       " 0.011913785710930824,\n",
       " -0.05828157439827919,\n",
       " -0.007259491831064224,\n",
       " -0.027569424360990524,\n",
       " 0.14092037081718445,\n",
       " 0.1645888239145279,\n",
       " 0.028781363740563393,\n",
       " -0.020195068791508675,\n",
       " 0.041742246598005295,\n",
       " -0.04870901629328728,\n",
       " -0.014992395415902138,\n",
       " -0.10004303604364395,\n",
       " 0.0010040032211691141,\n",
       " 0.007400031667202711,\n",
       " 0.033907875418663025,\n",
       " -0.046205244958400726,\n",
       " -0.05627479404211044,\n",
       " 0.014453720301389694,\n",
       " -0.04543150216341019,\n",
       " -0.02417799085378647,\n",
       " 0.02242404967546463,\n",
       " -0.00608924450352788,\n",
       " -0.03291724622249603,\n",
       " -0.051870111376047134,\n",
       " -0.04113011062145233,\n",
       " 0.023887744173407555,\n",
       " -0.03978649154305458,\n",
       " 0.05749962106347084,\n",
       " 0.0226025078445673,\n",
       " -0.00224814610555768,\n",
       " -0.014758797362446785,\n",
       " 0.02272529900074005,\n",
       " -0.024286732077598572,\n",
       " -0.04761049151420593,\n",
       " -0.04519468545913696,\n",
       " -0.017624735832214355,\n",
       " -0.04897991567850113,\n",
       " 0.01243257150053978,\n",
       " -0.013585059903562069,\n",
       " 0.007942629978060722,\n",
       " -0.0110758226364851,\n",
       " 0.029845384880900383,\n",
       " -0.010154175572097301,\n",
       " 0.03792734816670418,\n",
       " -0.008909889496862888,\n",
       " -0.019520005211234093,\n",
       " 0.04141175001859665,\n",
       " -0.05934157222509384,\n",
       " -0.003574049100279808,\n",
       " 0.005578226409852505,\n",
       " -0.029704149812459946,\n",
       " 0.01236009132117033,\n",
       " 0.024610351771116257,\n",
       " 0.03257439658045769,\n",
       " 0.0506843738257885,\n",
       " -0.0588935911655426,\n",
       " -0.02514098584651947,\n",
       " -0.010959312319755554,\n",
       " -0.0007855967269279063,\n",
       " 0.007951375097036362,\n",
       " 0.015620989724993706,\n",
       " 0.0003340133116580546,\n",
       " -0.023129690438508987,\n",
       " 0.03345072269439697,\n",
       " 0.05248858779668808,\n",
       " -0.001688167336396873,\n",
       " 0.032033924013376236,\n",
       " -0.05501792952418327,\n",
       " 0.0055666412226855755,\n",
       " -0.02116044983267784,\n",
       " 0.01517282985150814,\n",
       " -0.07774443924427032,\n",
       " -0.007919471710920334,\n",
       " -0.04720420762896538,\n",
       " -0.3026672601699829,\n",
       " 0.041370805352926254,\n",
       " -0.042589813470840454,\n",
       " 0.04632860794663429,\n",
       " 0.009057675488293171,\n",
       " 0.051354363560676575,\n",
       " -0.0029327876400202513,\n",
       " 0.036611054092645645,\n",
       " -0.0428994819521904,\n",
       " -0.008169656619429588,\n",
       " 0.0374789796769619,\n",
       " 0.0566423274576664,\n",
       " 0.03055930882692337,\n",
       " 0.028701985254883766,\n",
       " -0.0008734442526474595,\n",
       " 0.06365103274583817,\n",
       " 0.03292572498321533,\n",
       " -0.003685962175950408,\n",
       " 0.009021110832691193,\n",
       " -0.01473125908523798,\n",
       " -0.02675723284482956,\n",
       " 0.016145993024110794,\n",
       " 0.15721376240253448,\n",
       " -0.032471127808094025,\n",
       " 0.08464264869689941,\n",
       " 0.012344121932983398,\n",
       " -0.020810548216104507,\n",
       " 0.010183238424360752,\n",
       " 0.020970262587070465,\n",
       " -0.05673665553331375,\n",
       " 0.017366310581564903,\n",
       " -0.01617411896586418,\n",
       " 0.09107808768749237,\n",
       " -0.05155957117676735,\n",
       " 0.04161527752876282,\n",
       " -0.019468914717435837,\n",
       " 0.0029808147810399532,\n",
       " 0.0430067703127861,\n",
       " 0.014226154424250126,\n",
       " -0.01734613999724388,\n",
       " -0.03455725684762001,\n",
       " 0.0022605780977755785,\n",
       " -0.042772453278303146,\n",
       " 0.005511467345058918,\n",
       " 0.08570990711450577,\n",
       " -0.028510985895991325,\n",
       " -0.035712830722332,\n",
       " 0.0009942835895344615,\n",
       " -0.03844466432929039,\n",
       " 0.020116770640015602,\n",
       " -0.008065273053944111,\n",
       " 0.00991604384034872,\n",
       " -0.013173042796552181,\n",
       " 0.043020717799663544,\n",
       " 0.031553711742162704,\n",
       " 0.04707940295338631,\n",
       " -0.018580183386802673,\n",
       " -0.0201751496642828,\n",
       " -0.010406716726720333,\n",
       " -0.0013094404712319374,\n",
       " 0.013633926399052143,\n",
       " -0.01445348747074604,\n",
       " 0.031048012897372246,\n",
       " 0.06021175906062126,\n",
       " 0.049117546528577805]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BGE_query_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "my name is YoonSangCho\n"
     ]
    }
   ],
   "source": [
    "list_sim = [cos_sim(BGE_query_q, embeddings[c]) for c in range(len(embeddings))]\n",
    "idx_max_sim = np.where(list_sim==max(list_sim))[0][0]\n",
    "print(idx_max_sim)\n",
    "print(documents[0][idx_max_sim])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['my name is YoonSangCho',\n",
       "  'I majored in Industrial & Management Engineering',\n",
       "  'AI is powerful',\n",
       "  'where are you from?',\n",
       "  'Hello World']]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_max_sim\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. VectorStore**\n",
    "- embedding 처리 후 이들을 저장하는 벡터 저장소로 대표적 모듈로는 아래와 같다\n",
    "- (1) Chroma\n",
    "- (2) FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip isntall chromadb tiktoken transformers sentence_trannsformers faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multimodal\n",
      "Foundation\n",
      "ModelsSpecific-Purpose\n",
      "Pre-trained ModelsVisual\n",
      "Understanding §2Supervised LearningBiT (Kolesnikov et al., 2020);\n",
      "ViT (Dosovitskiy et al., 2021)\n",
      "Contrastive Language-\n",
      "Image Pre-trainingCLIP (Radford et al., 2021);\n",
      "ALIGN (Jia et al., 2021)\n",
      "Image-only Self-supervised LearningMoCo (He et al., 2020); DINO (Caron\n",
      "et al., 2021); MAE (He et al., 2022a)\n",
      "Synergy Among Different MethodsSLIP (Mu et al., 2021);\n",
      "UniCL (Yang et al., 2022b)\n",
      "Multimodal FusionUNITER (Chen et al., 2020d);\n",
      "CoCa (Yu et al., 2022a)\n",
      "Region-level and Pixel-\n",
      "level Pre-trainingGLIP (Li et al., 2022e);\n",
      "SAM (Kirillov et al., 2023)\n",
      "Visual\n",
      "Generation §3Overview: Text-to-Image GenerationStable Diffusion (Rom-\n",
      "bach et al., 2021)\n",
      "Spatial Controllable GenerationControlNet (Zhang\n",
      "and Agrawala, 2023)\n",
      "Text-based Editing InstructPix2Pix (Brooks et al., 2023)\n",
      "Text Prompts Following DDPO (Black et al., 2023)\n",
      "Concept Customization DreamBooth (Ruiz et al., 2023)\n",
      "General-Purpose\n",
      "AssistantsUnified Vision\n",
      "Models §4From Closed-set to Open-set ModelsGLIP (Li et al., 2022f);\n",
      "OpenSeg (Ghiasi et al., 2022b);\n",
      "OpenSeeD (Zhang et al., 2023e)\n",
      "From Task-Specific to Generic ModelsUnified-IO (Lu et al., 2022a);\n",
      "X-Decoder (Zou et al., 2023a)\n",
      "From Static to Promptable ModelsSAM (Kirillov et al., 2023);\n",
      "SEEM (Zou et al., 2023b);\n",
      "SegGPT (Wang et al., 2023j)\n",
      "Large Multi-\n",
      "modal Models:\n",
      "Training with\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"paper-0-multimodal.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0, length_function=tiktoken_len)\n",
    "docs = text_splitter.split_documents(pages)\n",
    "\n",
    "# create the open_source embedding function\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"jhgan/ko-sbert-nli\" # korean\n",
    "model_name = \"BAAI/bge-small-en\" # englilsh\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name = model_name,\n",
    "    model_kwargs = model_kwargs,\n",
    "    encode_kwargs = encode_kwargs\n",
    ")\n",
    "\n",
    "# 임베딩 벡터를 Chroma에 넣기\n",
    "db = Chroma.from_documents(docs, hf)\n",
    "\n",
    "# Query \n",
    "query  = \"What is the multomodal foundation models?\"\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "print(docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augmentations is required to guarantee predictive performancebecause, otherwise, abnormal symptoms may not become no-\n",
      "ticeable early enough to prevent an incident. It also necessitates\n",
      "data augmentations to enforce models to learn various scalesand patterns of abnormal signals.\n",
      "However, most existing approaches have largely focused\n",
      "on manual feature extraction methods of multisensor signals,revealing limitations for real-world applicability. Speciﬁcally,\n",
      "the traditional supervised learning approaches have overlooked\n",
      "data augmentation designed for time-series data in the contextof multisensor signals in manufacturing contexts. While recent\n",
      "studies have made strides in leveraging self-supervised learn-\n",
      "ing for representation learning, they have focused on utilizingunlabeled data, neglecting the potential beneﬁts of fully labeled\n",
      "data.\n",
      "Our study addresses these limitations by introducing super-\n",
      "vised contrastive learning (SCL) that leverages data augmenta-\n",
      "tions and contrastive learning to maximize the utility of fullylabeled data. Time-series augmentation [18] provides additional\n",
      "data of various scales and patterns, and contrastive learning\n",
      "based on labels increases the representation ability of class-wise features of multisensor signals. The SCL plays a role as\n",
      "the encoder pretraining strategy. It helps improve the model’s\n",
      "performance by initializing it with meaningful representations.Then, it is followed by classiﬁer training by transfer learning\n",
      "(with a frozen pretrained encoder) or ﬁne-tuning (with a trainable\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# (1) document loader\n",
    "loader = PyPDFLoader(\"paper-1-SCL.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# (2)  Text Splitting\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0, length_function = tiktoken_len)\n",
    "docs = text_splitter.split_documents(pages)\n",
    "\n",
    "# (3) embedding\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "model_name = \"all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "hf_scl = HuggingFaceEmbeddings(\n",
    "    model_name = model_name,\n",
    "    model_kwargs = model_kwargs,\n",
    "    encode_kwargs = encode_kwargs\n",
    ")\n",
    "# (4) Vector Store\n",
    "db = FAISS.from_documents(docs, hf_scl)\n",
    "query = \"what is the supervised contrastive learning?\"\n",
    "docs = db.similarity_search(query) # 유사도\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Retrieval을 통해 LLM에게 물어보기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yoonsangcho/opt/anaconda3/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To analyze the root cause of fault symptoms using multisensor signal data, you can follow these steps based on the information provided in the context:\n",
      "\n",
      "1. **Data Collection**: Utilize advanced sensor technology to collect real-time multisensor signal data related to the process or system you are analyzing. Ensure that the data includes information on sequential processes, processing times, and product quality.\n",
      "\n",
      "2. **Abnormal Signal Detection**: Detect abnormal signals within the collected sensor data. Abnormal signals are directly associated with abnormal process states and decreased product quality. These abnormal signals are crucial indicators of potential faults or issues.\n",
      "\n",
      "3. **Multilevel Cause Identification**: Identify multilevel causes by analyzing the sensor data. Problems often occur due to a combination of factors across different levels of the process. Understanding these multivariate processes and processing times is essential for identifying root causes accurately.\n",
      "\n",
      "4. **Temporary and Unknown Root Cause Detection**: The Root Cause Analysis (RCA) method should not only detect temporary causes but also uncover unknown root causes that inherently lead to abnormal processes. This involves going beyond surface-level issues to identify the fundamental reasons behind faults or symptoms.\n",
      "\n",
      "5. **Predictive Modeling**: Construct a predictive model based on the sensor data. This model should explain the relationship between process states, processing times, and product quality. By analyzing this predictive model, you can detect the root cause of faults by highlighting parameters that significantly impact the process.\n",
      "\n",
      "6. **Probabilistic or Deterministic Modeling**: Choose between probabilistic or deterministic modeling approaches for RCA. Probabilistic models, such as Bayesian networks, can provide probabilistic reasoning to analyze the sensor data effectively and identify root causes with a higher level of certainty.\n",
      "\n",
      "7. **Visualization and Analysis**: Generate Root Cause Maps (RCMs) based on the weighted sum of quality and causal maps derived from the sensor data. These maps can help visualize the relationship between process states, processing times, and quality-weighted activation scores. By analyzing these maps, you can pinpoint the major causes of abnormal quality and identify root cause sensors and processing times effectively.\n",
      "\n",
      "By following these steps and leveraging multisensor signal data along with advanced analytical techniques like predictive modeling and probabilistic reasoning, you can effectively analyze the root cause of fault symptoms in complex systems or processes."
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# document loader\n",
    "loader = PyPDFLoader(\"paper-2-QDL.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap = 50, length_function = tiktoken_len)\n",
    "texts = text_splitter.split_documents(pages)\n",
    "\n",
    "# embeddiing\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# model_name = \"all-mpnet-base-v2\"\n",
    "# model_name = \"jhgan/ko-sbert-nli\"\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name = model_name,\n",
    "    model_kwargs = model_kwargs,\n",
    "    encode_kwargs = encode_kwargs,\n",
    ")\n",
    "# VectorStore\n",
    "docsearch = Chroma.from_documents(texts, hf)\n",
    "\n",
    "# LLM\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "openai = ChatOpenAI(model_name = \"gpt-3.5-turbo\",\n",
    "                    streaming=True,\n",
    "                    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "                    temperature = 0\n",
    "                    )\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=openai,\n",
    "                                 chain_type=\"stuff\", ####\n",
    "                                 retriever = docsearch.as_retriever(\n",
    "                                     # chroma 객체를 벡터 저장소를 쓰는게 아니라 검색기로 쓰겠다\n",
    "                                     search_type=\"mmr\", # 연관성 높은 문서를 뽑아올 때 최대한 다양한 소스를 바탕으로 답변을 생성하도록 설정\n",
    "                                     search_kwargs={'k':3, 'fetch_k': 10}),# mmr 관련 변수\n",
    "                                 return_source_documents = True\n",
    "                                 )\n",
    "\n",
    "query = \"explain how to analyze the root cause of fault symtoms using multisensor signal data\"\n",
    "result = qa(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"]= None # 본인 OPENAI_API_KEY를 입력하세요\n",
    "import langchain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "**1. ChatGPT 모델을 불러오고 사용자 선택변수를 조정하여 답변을 비교해보세요.**\n",
    "- SystemMessage, HumanMessage를 변경하여 ChatGPT 역할을 부여하고 원하는 형식의 답변을 유도해보세요.\n",
    "- 사용자 선택변수 model_name, temperature을 조정하여 답변 차이를 비교해보세요.\n",
    "\"\"\"\n",
    "chatgpt = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "message = [\n",
    "    SystemMessage(\n",
    "        content=None # ChatGPT에게 역할을 부여하세요\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=None # ChatGPT에게 물어보고 싶은 내용을 입력하세요.\n",
    "    )\n",
    "]\n",
    "answer = None # chatgpt와 지정한 message를 이용하여 답변을 출력하세요. 이때, chatgpt가 답변한 내용(content) 만 출력하세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (222686886.py, line 47)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[49], line 47\u001b[0;36m\u001b[0m\n\u001b[0;31m    streaming=True,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\" \n",
    "**2. ChatGPT 모델을 불러오고 원하는 PDF 데이터를 입력하여 정보를 검색할 수 있도록 ChatGPT를 구축하세요.**\n",
    "- 본인 현업에서 가장 적용하고 싶은 AI기술을 다룬 논문을 찾아 로드하세요.\n",
    "- ChatOpenAI 모듈에서 model_name 조정하여 답변 차이를 비교해보세요.\n",
    "- RetrievalQA.from_chain_type 모듈에서 chain_type 조정하여 답변 차이를 비교해보세요.\n",
    "\"\"\"\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "# (1) Document loaders\n",
    "loader = None # 본인이 원하는 PDF를 로드하세요.\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# (2) Splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap = 50, length_function = tiktoken_len)\n",
    "texts = text_splitter.split_documents(pages)\n",
    "\n",
    "# (3) Embeddinng\n",
    "model_name = None # 본인이 원하는 EMBEDDING model_name 지정하세요.\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name = model_name,\n",
    "    model_kwargs = model_kwargs,\n",
    "    encode_kwargs = encode_kwargs,\n",
    ")\n",
    "\n",
    "# (4) VectorStores\n",
    "docsearch = Chroma.from_documents(texts, hf)\n",
    "\n",
    "# (5) LLM\n",
    "openai = ChatOpenAI(model_name = None # 본인이 원하는 LLM model_name 지정하세요.\n",
    "                    streaming=True,\n",
    "                    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "                    temperature = 0\n",
    "                    )\n",
    "\n",
    "# (6) RetrievalQA\n",
    "qa = RetrievalQA.from_chain_type(llm=openai,\n",
    "                                 chain_type= None, # chain_type을 지정하세요\n",
    "                                 retriever = None, # retriever 지정하세요\n",
    "                                 return_source_documents = None, # return_source_documents 지정하세요\n",
    "                                 )\n",
    "\n",
    "query = None # 본인 직무에 적용할 수 있는 방법론과 관련된 질문을 ChatGPT에게 물어보세요.\n",
    "result = qa(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
